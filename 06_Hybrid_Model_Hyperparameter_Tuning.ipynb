{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d491748",
   "metadata": {},
   "source": [
    "# k-means Subsegment Models + Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cef384",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle \n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, precision_recall_curve\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, roc_auc_score, f1_score\n",
    "from sklearn.metrics import accuracy_score, average_precision_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import lightgbm as lgb\n",
    "from imblearn.pipeline import Pipeline as pipe_imb\n",
    "from imblearn.over_sampling import SMOTE, SMOTENC, RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "from imblearn.pipeline import make_pipeline as make_pipeline_imb\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster import hierarchy\n",
    "from sklearn import compose\n",
    "from sklearn import neighbors\n",
    "from sklearn import decomposition\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "def f1_macro(y_true, y_pred):\n",
    "    return f1_score(y_true, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940682d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data frame to store the results\n",
    "def print_results(headline, true_value, pred, probs):\n",
    "    scores=[]\n",
    "    CM = confusion_matrix(true_value, pred)\n",
    "    scores.append(headline)\n",
    "    scores.append(accuracy_score(true_value, pred))      #accuracy\n",
    "    scores.append(int(CM[1,1]))                          #TP\n",
    "    scores.append(int(CM[0,1]))                          #FP\n",
    "    scores.append(int(CM[0][0]))                         #TN\n",
    "    scores.append(int(CM[1][0]))                         #FN\n",
    "    scores.append(precision_score(true_value, pred))     #precision\n",
    "    scores.append(recall_score(true_value, pred))        #recall\n",
    "    scores.append(roc_auc_score(true_value, probs))      #roc_auc\n",
    "    p, r, _ = precision_recall_curve(true_value, probs) \n",
    "    scores.append(auc(r,p))                              #pr_auc\n",
    "    scores.append(f1_score(true_value, pred, average=\"macro\"))            #f1-score\n",
    "    return scores\n",
    "\n",
    "score_names = ['method','accuracy','TP','FP','TN','FN','precision','recall','roc_auc','pr_auc','f1']\n",
    "dfAcc = dfAcc = pd.DataFrame(data=np.zeros(shape=(0,11)), columns = score_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d637b2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAcc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b220190",
   "metadata": {},
   "source": [
    "# Import data subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de84e180",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "fd = open(\"D5_datasets_kmeans\", 'rb') \n",
    "clu_datalist = pickle.load(fd)\n",
    "fd.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fafa7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X4_0 = clu_datalist[0]\n",
    "X4_1 = clu_datalist[1]\n",
    "X4_2 = clu_datalist[2]\n",
    "X4_3 = clu_datalist[3]\n",
    "\n",
    "y4_0 = clu_datalist[4]\n",
    "y4_1 = clu_datalist[5]\n",
    "y4_2 = clu_datalist[6]\n",
    "y4_3 = clu_datalist[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8fd614",
   "metadata": {},
   "source": [
    "## Prediction by using cluster-based data subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6e52d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test train split for each subset\n",
    "\n",
    "X4_0_train, X4_0_test, y4_0_train, y4_0_test = train_test_split(X4_0, y4_0, test_size=0.33, random_state=42)\n",
    "X4_1_train, X4_1_test, y4_1_train, y4_1_test = train_test_split(X4_1, y4_1, test_size=0.33, random_state=42)\n",
    "X4_2_train, X4_2_test, y4_2_train, y4_2_test = train_test_split(X4_2, y4_2, test_size=0.33, random_state=42)\n",
    "X4_3_train, X4_3_test, y4_3_train, y4_3_test = train_test_split(X4_3, y4_3, test_size=0.33, random_state=42)\n",
    "\n",
    "\n",
    "# for sake of efficiency, create a matrix\n",
    "\n",
    "cluster_train_test_matrix = [[X4_0_train, X4_0_test, y4_0_train, y4_0_test],\n",
    "                                [X4_1_train, X4_1_test, y4_1_train, y4_1_test],\n",
    "                                [X4_2_train, X4_2_test, y4_2_train, y4_2_test],\n",
    "                                [X4_3_train, X4_3_test, y4_3_train, y4_3_test]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c2ea90",
   "metadata": {},
   "source": [
    "### Tune cluster 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d129027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune\n",
    "\n",
    "ovsmp_pipe = pipe_imb([    ('imputer'   , SimpleImputer(strategy=\"median\")),\n",
    "                           ('scaler'    , StandardScaler()),\n",
    "                           ('sampler'   , RandomOverSampler(random_state=42,\n",
    "                                                            sampling_strategy = 0.6)),\n",
    "                           ('classifier', XGBClassifier(objective='binary:logistic',\n",
    "                                                        eval_metric = f1_macro,\n",
    "                                                        n_estimators=1000, \n",
    "                                                        eta=0.005, \n",
    "                                                        max_depth=3, \n",
    "                                                        subsample=0.5, \n",
    "                                                        min_child_weight=50, \n",
    "                                                       gamma=5, \n",
    "                                                        reg_lambda=0.5, \n",
    "                                                        alpha=6, \n",
    "                                                        colsample_bytree=0.5 \n",
    "                                                       ))\n",
    "                 ])\n",
    "\n",
    "params = [{\n",
    "          #  'sampler__sampling_strategy': [0.5,0.6,0.7,0.8,0.9],\n",
    "          #  'classifier__n_estimators':[1000,1300,1500],\n",
    "          #  'classifier__eta': [0.01,0.005,0.0001],\n",
    "          #   'classifier__max_depth':[2,3,4],\n",
    "         #  'classifier__min_child_weight': [50,100,150],\n",
    "         #  'classifier__colsample_bytree':[0.5,0.7,1],\n",
    "         #  'classifier__subsample' : [0.5,0.6,0.7,0.8,0.9],\n",
    "         #  'classifier__alpha':[4,5,6], #  defult 0. Increasing this value will make model more conservative.\n",
    "          #  'classifier__gamma':[4,5,6], #  defult 0. Increasing this value will make model more conservative.\n",
    "         #   'classifier__reg_lambda':[0.5,1,1.5], #  def=1 .Increasing this value will make model more conservative.\n",
    "         #   'classifier__scale_pos_weight' : [1]\n",
    "}\n",
    "         ]\n",
    "\n",
    "# train/validation with the same ratio of classes\n",
    "kfold = StratifiedKFold(n_splits = 4, random_state = 42, shuffle=True) \n",
    "\n",
    "grid = GridSearchCV(ovsmp_pipe, \n",
    "                    param_grid = params, \n",
    "                    cv = kfold, \n",
    "                    scoring = 'f1_macro', \n",
    "                    verbose = 1,\n",
    "                    n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa0a496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the grid search \n",
    "\n",
    "\n",
    "grid.fit(X4_0_train, y4_0_train)\n",
    "print(grid.best_estimator_,'\\n')\n",
    "print('Best parameters  :', grid.best_params_)\n",
    "print('\\nTraining F1_macro:', grid.score(X4_0_train, y4_0_train))\n",
    "print('Test F1_macro :', grid.score(X4_0_test, y4_0_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec52cfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print results\n",
    "\n",
    "pred_probs = grid.predict_proba(X4_0_test)[:,1]\n",
    "scores = print_results('D5_XGB_km-clu0_oversmp', y4_0_test, grid.predict(X4_0_test), pred_probs)\n",
    "dftmp = pd.DataFrame([scores], columns=score_names)\n",
    "dfAcc = pd.concat([dfAcc, dftmp], ignore_index=True)\n",
    "dfAcc[['TP','FP', 'TN', 'FN']] = dfAcc[['TP','FP', 'TN', 'FN']].astype(int)\n",
    "dfAcc.style.hide_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394670b9",
   "metadata": {},
   "source": [
    "## Tune cluster 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b37882c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune\n",
    "\n",
    "ovsmp_pipe = pipe_imb([  ('imputer'   , SimpleImputer(strategy=\"median\")),\n",
    "                         ('scaler'    , StandardScaler()),\n",
    "                         ('sampler'   , RandomOverSampler(random_state=42,\n",
    "                                                        sampling_strategy = 0.8)),\n",
    "                         ('classifier', XGBClassifier(objective='binary:logistic',\n",
    "                                                        eval_metric = f1_macro,\n",
    "                                                        n_estimators=1300, \n",
    "                                                        eta=0.005, \n",
    "                                                        max_depth=5, \n",
    "                                                        subsample=0.9, \n",
    "                                                        min_child_weight=150, \n",
    "                                                        gamma=5, \n",
    "                                                        reg_lambda=1, \n",
    "                                                        alpha=5, \n",
    "                                                        colsample_bytree=0.6, \n",
    "                                                       ))\n",
    "                 ])\n",
    "\n",
    "params = [{\n",
    "           # 'sampler__sampling_strategy': [0.5,0.6,0.7,0.8,0.9],\n",
    "         #   'classifier__n_estimators':[1200,1300,1400],\n",
    "          #  'classifier__eta': [0.01,0.05,0.1,0.005],\n",
    "          #   'classifier__max_depth':[4,5,6],\n",
    "          # 'classifier__min_child_weight': [125,150,175],\n",
    "          # 'classifier__colsample_bytree':[0.5,0.6,0.7,0.8,0.9,1],\n",
    "        #   'classifier__subsample' : [0.5,0.6,0.7,0.8,0.9,1],\n",
    "         #  'classifier__alpha':[1,3,5], #  defult 0. Increasing this value will make model more conservative.\n",
    "          #  'classifier__gamma':[1,3,5], #  defult 0. Increasing this value will make model more conservative.\n",
    "         #   'classifier__reg_lambda':[0.5,1,1.5], #  def=1 .Increasing this value will make model more conservative.\n",
    "         #   'classifier__scale_pos_weight' : [1]\n",
    "}\n",
    "         ]\n",
    "\n",
    "# train/validation with the same ratio of classes\n",
    "kfold = StratifiedKFold(n_splits = 4, random_state = 42, shuffle=True) \n",
    "\n",
    "grid = GridSearchCV(ovsmp_pipe, \n",
    "                    param_grid = params, \n",
    "                    cv = kfold, \n",
    "                    scoring = 'f1_macro', \n",
    "                    verbose = 1,\n",
    "                    n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dce06f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the grid search \n",
    "\n",
    "grid.fit(X4_1_train, y4_1_train)\n",
    "print(grid.best_estimator_,'\\n')\n",
    "print('Best parameters  :', grid.best_params_)\n",
    "print('\\nTraining F1_macro:', grid.score(X4_1_train, y4_1_train))\n",
    "print('Test F1_macro :', grid.score(X4_1_test, y4_1_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542fe077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print results\n",
    "\n",
    "pred_probs = grid.predict_proba(X4_1_test)[:,1]\n",
    "scores = print_results('D5_XGB_km-clu1_oversmp', y4_1_test, grid.predict(X4_1_test), pred_probs)\n",
    "dftmp = pd.DataFrame([scores], columns=score_names)\n",
    "dfAcc = pd.concat([dfAcc, dftmp], ignore_index=True)\n",
    "dfAcc[['TP','FP', 'TN', 'FN']] = dfAcc[['TP','FP', 'TN', 'FN']].astype(int)\n",
    "dfAcc.style.hide_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec07c23",
   "metadata": {},
   "source": [
    "## Tune cluster 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1f1e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune\n",
    "\n",
    "ovsmp_pipe = pipe_imb([      ('imputer'   , SimpleImputer(strategy=\"median\")),\n",
    "                       ('scaler'    , StandardScaler()),\n",
    "                        ('sampler'   , RandomOverSampler(random_state=42,\n",
    "                                                         sampling_strategy = 0.4)),\n",
    "                       ('classifier', XGBClassifier(objective='binary:logistic',\n",
    "                                                    eval_metric = f1_macro,\n",
    "                                                    n_estimators=1300, \n",
    "                                                    eta=0.02, \n",
    "                                                    max_depth=3, \n",
    "                                                    subsample=0.9, \n",
    "                                                    min_child_weight=150, \n",
    "                                                    gamma=3, \n",
    "                                                    reg_lambda=1, \n",
    "                                                    alpha=5, \n",
    "                                                    colsample_bytree=0.7, \n",
    "                                                   ))\n",
    "                 ])\n",
    "\n",
    "params = [{\n",
    "           # 'sampler__sampling_strategy': [0.3,0.4,0.5],\n",
    "          #   'classifier__n_estimators':[1200,1300,1400],\n",
    "          #  'classifier__eta': [0.01,0.02,0.005,0.0001],\n",
    "          #   'classifier__max_depth':[3,5,7],\n",
    "         #  'classifier__min_child_weight': [50,100,150],\n",
    "          # 'classifier__colsample_bytree':[0.5,0.6,0.7,0.8,0.9,1],\n",
    "          # 'classifier__subsample' : [0.5,0.6,0.7,0.8,0.9,1],\n",
    "          # 'classifier__alpha':[1,3,5], #  defult 0. Increasing this value will make model more conservative.\n",
    "        #    'classifier__gamma':[0,1,3,5], #  defult 0. Increasing this value will make model more conservative.\n",
    "           # 'classifier__reg_lambda':[0,1,3,5], #  def=1 .Increasing this value will make model more conservative.\n",
    "         #   'classifier__scale_pos_weight' : [1]\n",
    "}\n",
    "         ]\n",
    "\n",
    "# train/validation with the same ratio of classes\n",
    "kfold = StratifiedKFold(n_splits = 4, random_state = 42, shuffle=True) \n",
    "\n",
    "grid = GridSearchCV(ovsmp_pipe, \n",
    "                    param_grid = params, \n",
    "                    cv = kfold, \n",
    "                    scoring = 'f1_macro', \n",
    "                    verbose = 1,\n",
    "                    n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e426c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the grid search \n",
    "\n",
    "grid.fit(X4_2_train, y4_2_train)\n",
    "print(grid.best_estimator_,'\\n') \n",
    "print('Best parameters  :', grid.best_params_)\n",
    "print('\\nTraining F1_macro:', grid.score(X4_2_train, y4_2_train))\n",
    "print('Test F1_macro :', grid.score(X4_2_test, y4_2_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8087903f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print results\n",
    "\n",
    "pred_probs = grid.predict_proba(X4_2_test)[:,1]\n",
    "scores = print_results('D5_XGB_km-clu2_oversmp', y4_2_test, grid.predict(X4_2_test), pred_probs)\n",
    "dftmp = pd.DataFrame([scores], columns=score_names)\n",
    "dfAcc = pd.concat([dfAcc, dftmp], ignore_index=True)\n",
    "dfAcc[['TP','FP', 'TN', 'FN']] = dfAcc[['TP','FP', 'TN', 'FN']].astype(int)\n",
    "dfAcc.style.hide_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222fe098",
   "metadata": {},
   "source": [
    "## Tune cluster 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2129a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune\n",
    "\n",
    "ovsmp_pipe = pipe_imb([      ('imputer'   , SimpleImputer(strategy=\"median\")),\n",
    "                       ('scaler'    , StandardScaler()),\n",
    "                      ('sampler'   , RandomOverSampler(random_state=42,\n",
    "                                    sampling_strategy = 1)),\n",
    "                       ('classifier', XGBClassifier(objective='binary:logistic',\n",
    "                                                    eval_metric = f1_macro,\n",
    "                                                    n_estimators=1000, \n",
    "                                                    eta=0.005, \n",
    "                                                    max_depth=3, \n",
    "                                                    subsample=0.7, \n",
    "                                                    min_child_weight=100, \n",
    "                                                    gamma=5, \n",
    "                                                    reg_lambda=3, \n",
    "                                                    alpha=5, \n",
    "                                                    colsample_bytree=0.5, \n",
    "                                                   ))\n",
    "                 ])\n",
    "\n",
    "params = [{\n",
    "           # 'sampler__sampling_strategy': [0,1],\n",
    "          #  'classifier__n_estimators':[950,1000,1050],\n",
    "           # 'classifier__eta': [0.01,0.1,0.05,0.005],\n",
    "             'classifier__max_depth':[2],\n",
    "         #  'classifier__min_child_weight': [50,100,150],\n",
    "         #  'classifier__colsample_bytree':[0.5,0.6,0.7,0.8,0.9,1],\n",
    "          # 'classifier__subsample' : [0.5,0.6,0.7,0.8,0.9,1],\n",
    "          # 'classifier__alpha':[1,3,5], #  defult 0. Increasing this value will make model more conservative.\n",
    "         #   'classifier__gamma':[0,1,3,5], #  defult 0. Increasing this value will make model more conservative.\n",
    "         #   'classifier__reg_lambda':[2,3,4], #  def=1 .Increasing this value will make model more conservative.\n",
    "         #   'classifier__scale_pos_weight' : [1]\n",
    "}\n",
    "         ]\n",
    "\n",
    "# train/validation with the same ratio of classes\n",
    "kfold = StratifiedKFold(n_splits = 4, random_state = 42, shuffle=True) \n",
    "\n",
    "grid = GridSearchCV(ovsmp_pipe, \n",
    "                    param_grid = params, \n",
    "                    cv = kfold, \n",
    "                    scoring = 'f1_macro', \n",
    "                    verbose = 1,\n",
    "                    n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f48ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the grid search \n",
    "\n",
    "grid.fit(X4_3_train, y4_3_train)\n",
    "print(grid.best_estimator_,'\\n')\n",
    "print('Best parameters  :', grid.best_params_)\n",
    "print('\\nTraining F1_macro:', grid.score(X4_3_train, y4_3_train))\n",
    "print('Test F1_macro :', grid.score(X4_3_test, y4_3_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fcb38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Best parameters  : {'classifier__colsample_bytree': 0.9, 'classifier__eta': 0.01, 'classifier__n_estimators': 1500, 'classifier__subsample': 0.9}\n",
    "\n",
    "Training F1_macro: 0.8436718625554671\n",
    "Test F1_macro : 0.812172183034907\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40060ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print results\n",
    "\n",
    "pred_probs = grid.predict_proba(X4_3_test)[:,1]\n",
    "scores = print_results('D5_XGB_km-clu3_oversmp', y4_3_test, grid.predict(X4_3_test), pred_probs)\n",
    "dftmp = pd.DataFrame([scores], columns=score_names)\n",
    "dfAcc = pd.concat([dfAcc, dftmp], ignore_index=True)\n",
    "dfAcc[['TP','FP', 'TN', 'FN']] = dfAcc[['TP','FP', 'TN', 'FN']].astype(int)\n",
    "dfAcc.style.hide_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e03fdd",
   "metadata": {},
   "source": [
    "## Permutation Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e00e48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "r = permutation_importance(grid, X4_3_train, y4_3_train,\n",
    "                               n_repeats=30,\n",
    "                             random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f509c4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(r.importances);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a316e8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "r.importances_mean[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa260828",
   "metadata": {},
   "outputs": [],
   "source": [
    "X4_3_train.columns[9]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
