{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55a4f3f1",
   "metadata": {},
   "source": [
    "# 02_Exploratory Data Analysis & Baseline Performances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5889f86a",
   "metadata": {},
   "source": [
    "**Author:** Bilge Nur Karaca\n",
    "\n",
    "*Feature names used in this project are either altered or invented and do not represent the original feature names. Code outputs are not provided due to confidentiality.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b9b629",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle \n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f805f453",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('OM_D1_train_data', 'rb')\n",
    "df = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b017fd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.label.value_counts())\n",
    "print(len(df))\n",
    "print(df.shape)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e4f0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_vars = ['num_of_transactions', 'avg_basket_size', 'num_of_total_items',\n",
    "               'min_basket_size', 'max_basket_size', 'avg_discount_amount',\n",
    "               'lifetime_discount_amount', 'num_of_purchases_w_discount',\n",
    "               'avg_order_value', 'total_net_amount', 'isShippedToBilled_sum',\n",
    "               'days_since_last_purchase', 'days_since_first_purchase', 'num_returns',\n",
    "               'Category1', 'Category2', 'Category3', 'Category4', 'Axe1',\n",
    "               'Axe2', 'Axe3', 'Axe4', 'Age', 'purchase_frequency', 'offline_transactions']\n",
    "\n",
    "cat_vars = ['isContactable','City' ,'last_coupon_type_used','Device']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa44a4d",
   "metadata": {},
   "source": [
    "### Draw histograms for all numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8db59de",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Draw histograms for all numerical features\n",
    "\n",
    "plt.rc('xtick', labelsize=6) \n",
    "plt.rc('ytick', labelsize=6)\n",
    "df[num_vars].hist(figsize = [20,15], bins=30);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc804ad",
   "metadata": {},
   "source": [
    "### Null value adjustments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3b57ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make \"Age\" null if the value is smaller than 18 and greater than 100.\n",
    "\n",
    "df[\"Age\"] = df.loc[:,\"Age\"].apply(lambda x: np.where(x<18,np.nan,x))\n",
    "df[\"Age\"] = df.loc[:,\"Age\"].apply(lambda x: np.where(x>100,np.nan,x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9bef16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make purhcase frequency \"0\" to \"Null\" because those are single-purchasers. \n",
    "# \"0-frequency\" makes no sense.\n",
    "\n",
    "df[\"purchase_freq\"].replace(0,np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90dbfb9d",
   "metadata": {},
   "source": [
    "### Remove outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0aa12cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "colns = [\"avg_order_value\", 'Category1', 'Category2', 'Category3', 'Category4', 'Axe1',\n",
    "               'Axe2', 'Axe3', 'Axe4',\"offline_transactions\", \"num_returns\",\n",
    "            'isShippedToBilled_sum', \"avg_discount_amount\", \"avg_basket_size\"]\n",
    "\n",
    "for col in colns:\n",
    "    print(df[col].apply(lambda x: x > df[col].mean() + 6*df[col].std()).sum(),\"outliers, 6 std far from the mean\")\n",
    "    print(df[col].apply(lambda x: x > df[col].mean() + 6*df[col].std()).sum() /df[col].count(),\"of the column\")\n",
    "    print(\"Dropping...\")\n",
    "    df.drop(df[df[col].apply(lambda x: x > df[col].mean() + 6*df[col].std())==True].index, inplace=True)\n",
    "    print(\"Dropped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab39dcf",
   "metadata": {},
   "source": [
    "### Recheck histograms after outlier removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde7b103",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Recheck histograms after outlier removal\n",
    "\n",
    "plt.rc('xtick', labelsize=6) \n",
    "plt.rc('ytick', labelsize=6)\n",
    "df[num_vars].hist(figsize = [20,15], bins=30);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7852bd",
   "metadata": {},
   "source": [
    "#### Add a new feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2908dc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the inverse of frequence (i.e. period) as a new feature\n",
    "\n",
    "df[\"purchase_period\"] = df[\"purchase_freq\"]\n",
    "df[\"purchase_freq\"] = 1/df[\"purchase_freq\"]\n",
    "\n",
    "num_vars = num_vars + [\"purchase_period\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d2380b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['purchase_period'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f607af67",
   "metadata": {},
   "outputs": [],
   "source": [
    "col= \"purchase_freq\"\n",
    "print(df[col].apply(lambda x: x > df[col].mean() + 9*df[col].std()).sum(),\"outliers, 6 std far from the mean\")\n",
    "print(df[col].apply(lambda x: x > df[col].mean() + 9*df[col].std()).sum() /df[col].count(),\"of the column\")\n",
    "print(\"Dropping...\")\n",
    "df.drop(df[df[col].apply(lambda x: x > df[col].mean() + 9*df[col].std())==True].index, inplace=True)\n",
    "print(\"Dropped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216fff60",
   "metadata": {},
   "source": [
    "### Class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bee1382",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"# of samples per class:\", df.label.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b38a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class imbalance\n",
    "df.label.value_counts()[1]/(df.label.value_counts()[1]+df.label.value_counts()[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378a1015",
   "metadata": {},
   "source": [
    "### Check correlation matrix by heatmap visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737c3233",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set()\n",
    "plt.figure(figsize=(25,25))\n",
    "plt.rc('xtick', labelsize=20) \n",
    "plt.rc('ytick', labelsize=20)\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "sns.heatmap(round(df[num_vars + [\"label\"]].corr(),2), annot=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcbfc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop highly correlated variables\n",
    "\n",
    "df.drop(\"Category2\", axis=1,inplace=True) # 1 correlation\n",
    "df.drop(\"isShippedToBilled_sum\", axis=1,inplace=True)# 1 correlation\n",
    "df.drop(\"num_of_purchases_w_discount\", axis=1,inplace=True) # 1 correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de1591b",
   "metadata": {},
   "source": [
    "### Check categorical variables & visualize distributions with regard to churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1425bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check categorical variables\n",
    "\n",
    "df.describe(include = 'object').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6ab736",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualize how the churn rate changes for different types of device.\n",
    "\n",
    "pd.crosstab(df[\"Device\"],df['label']).plot.bar(stacked=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69f3444",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualize how the value distributions are for cat_vars.\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "\n",
    "idx = 1\n",
    "for cat in cat_vars: \n",
    "    plt.subplot(3, 3, idx)\n",
    "    sns.countplot(df[cat], label=\"Count\")\n",
    "    \n",
    "    plt.xticks(rotation=90)\n",
    "\n",
    "    #plt.xlabel(cat, fontsize=20)\n",
    "    idx += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02f489b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isContactable = df.isContactable.astype(\"category\")\n",
    "df.City = df.City.astype(\"category\")\n",
    "df.last_coupon_type_used = df.last_coupon_type_used.astype(\"category\")\n",
    "df.Device = df.Device.astype(\"category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b70940",
   "metadata": {},
   "source": [
    "### Visualize numericals variable distributions with regard to churn (1 vs. 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477a1596",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualize how the boxplots are for num_vars, for each label value.\n",
    "\n",
    "plt.figure(figsize=(20,40))\n",
    "for i, col in enumerate(df[num_vars]):\n",
    "    plt.subplot(6,4,i+1)\n",
    "    sns.boxplot(x='label', y=col, data=df)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a183b3c5",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47afd9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab06759",
   "metadata": {},
   "outputs": [],
   "source": [
    "X= df.drop([\"CustomerID\",\"label\"],axis=1)\n",
    "y= df[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfba113",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731fc0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7acb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.drop(cat_vars, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18433813",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26f2e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2f7ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"pipe_cat = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"one_hot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\"\"\"\n",
    "pipe_num = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "ct = ColumnTransformer([\n",
    "    #(\"categorical_vars\", pipe_cat, cat_vars),\n",
    "    (\"numeric_vars\", pipe_num, num_vars),\n",
    "    \n",
    "], remainder=\"passthrough\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a0f03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d807bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = df.copy()\n",
    "import pickle \n",
    "fx = open(\"OM_D2_Train_Data_Cleaned\", 'wb') \n",
    "pickle.dump(clean_data, fx)\n",
    "fx.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6d2fba",
   "metadata": {},
   "source": [
    "# Results Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc37cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, precision_recall_curve\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, roc_auc_score, f1_score\n",
    "from sklearn.metrics import accuracy_score, average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d9c4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data frame to store the results\n",
    "def print_results(headline, true_value, pred, probs):\n",
    "    scores=[]\n",
    "    CM = confusion_matrix(true_value, pred)\n",
    "    scores.append(headline)\n",
    "    scores.append(accuracy_score(true_value, pred))      #accuracy\n",
    "    scores.append(int(CM[1,1]))                          #TP\n",
    "    scores.append(int(CM[0,1]))                          #FP\n",
    "    scores.append(int(CM[0][0]))                         #TN\n",
    "    scores.append(int(CM[1][0]))                         #FN\n",
    "    scores.append(precision_score(true_value, pred))     #precision\n",
    "    scores.append(recall_score(true_value, pred))        #recall\n",
    "    scores.append(roc_auc_score(true_value, probs))      #roc_auc\n",
    "    p, r, _ = precision_recall_curve(true_value, probs) \n",
    "    scores.append(auc(r,p))                              #pr_auc\n",
    "    scores.append(f1_score(true_value, pred, average=\"macro\"))            #f1-score\n",
    "    return scores\n",
    "\n",
    "score_names = ['method','accuracy','TP','FP','TN','FN','precision','recall','roc_auc','pr_auc','f1']\n",
    "dfAcc = pd.DataFrame(data=np.zeros(shape=(0,11)), columns = score_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90aa1148",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200cc677",
   "metadata": {},
   "source": [
    "# LoR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4aca73",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(steps=[('ct', ct),\n",
    "                       ('classifier', LogisticRegression(random_state=42, max_iter=100000))\n",
    "                          ]\n",
    "                   )\n",
    "\n",
    "params = [{'classifier__C'      : [0.3,0.4,0.5,0.6,0.7,0.8,0.9,1], \n",
    "           'classifier__penalty': ['l1'],\n",
    "           'classifier__solver' : ['liblinear', 'saga']},\n",
    "          \n",
    "          {'classifier__C'      : [0.3,0.4,0.5,0.6,0.7,0.8,0.9,1], \n",
    "           'classifier__penalty': ['l2'], \n",
    "           'classifier__solver' : ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga']\n",
    "          }\n",
    "         ]\n",
    "\n",
    "# train/validation with the same ratio of classes\n",
    "kfold = StratifiedKFold(n_splits = 4, random_state = 42, shuffle=True) \n",
    "\n",
    "grid = GridSearchCV(pipe, \n",
    "                    param_grid = params, \n",
    "                    cv = kfold, \n",
    "                    scoring = 'f1_macro', \n",
    "                    verbose = 1,\n",
    "                    n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edb87c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Run the grid search \n",
    "grid.fit(X_train, y_train)\n",
    "print(grid.best_estimator_,'\\n')\n",
    "print('Best parameters  :', grid.best_params_)\n",
    "print('\\nTraining F1_macro:', grid.score(X_train, y_train))\n",
    "print('Test F1_macro :', grid.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd6032b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probs = grid.predict_proba(X_test)[:,1]\n",
    "scores = print_results('D2_LoR_nt', y_test, grid.predict(X_test), pred_probs)\n",
    "dftmp = pd.DataFrame([scores], columns=score_names)\n",
    "dfAcc = pd.concat([dfAcc, dftmp], ignore_index=True)\n",
    "dfAcc[['TP','FP', 'TN', 'FN']] = dfAcc[['TP','FP', 'TN', 'FN']].astype(int)\n",
    "dfAcc.style.hide_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9893c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_macro(y_true, y_pred):\n",
    "    return f1_score(y_true, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03db47ff",
   "metadata": {},
   "source": [
    "# XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1816e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGB\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "pipe = Pipeline(steps=[('ct', ct),\n",
    "                       ('classifier', XGBClassifier(objective='binary:logistic', \n",
    "                                                    eval_metric = f1_macro,\n",
    "                                                    n_estimators=1000, \n",
    "                                                    eta=0.01, # default 0.3\n",
    "                                                    max_depth=8, \n",
    "                                                    subsample=0.7, \n",
    "                                                    min_child_weight=55, \n",
    "                                                    gamma=1, \n",
    "                                                    reg_lambda=1, \n",
    "                                                    alpha=1, \n",
    "                                                    colsample_bytree=0.9, \n",
    "                                                    #colsample_bylevel=0.5,\n",
    "                                                   # scale_pos_weight = 0.35\n",
    "                                                   )\n",
    "                                                   )\n",
    "                            ]\n",
    "                   )\n",
    "\n",
    "params = [{ \n",
    "            #'classifier__n_estimators':[750,1000,1500],\n",
    "           #'classifier__eta': [0.01],\n",
    "           # 'classifier__max_depth':[8,9,10],\n",
    "          # 'classifier__min_child_weight': [50,55,60],\n",
    "         #  'classifier__colsample_bytree':[0.7,0.8,0.9],\n",
    "           # 'classifier__subsample' : [0.6,0.7,0.8],\n",
    "         #  'classifier__alpha':[0.2,0.3,0.4], #  defult 0. Increasing this value will make model more conservative.\n",
    "        #    'classifier__gamma':[0.2,0.3,0.4], #  defult 0. Increasing this value will make model more conservative.\n",
    "          #  'classifier__reg_lambda':[0.5,1,1.5,2,2.5], #  def=1 .Increasing this value will make model more conservative.\n",
    "          #  'classifier__scale_pos_weight' : [0.25,0.3,0.35]\n",
    "          }\n",
    "         ]\n",
    "\n",
    "# train/validation with the same ratio of classes\n",
    "kfold = StratifiedKFold(n_splits = 4, random_state = 42, shuffle=True) \n",
    "\n",
    "grid = GridSearchCV(pipe, \n",
    "                    param_grid = params, \n",
    "                    cv = kfold, \n",
    "                    scoring = 'f1_macro', \n",
    "                    verbose = 1,\n",
    "                    n_jobs = -1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1198d2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Run the grid search \n",
    "grid.fit(X_train, y_train)\n",
    "print(grid.best_estimator_,'\\n')\n",
    "print('Best parameters  :', grid.best_params_)\n",
    "print('\\nTraining F1 score:', grid.score(X_train, y_train))\n",
    "print(\"Test F1 score:\", grid.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7061a597",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probs = grid.predict_proba(X_test)[:,1]\n",
    "scores = print_results('D2_XGB_nt', y_test, grid.predict(X_test), pred_probs)\n",
    "dftmp = pd.DataFrame([scores], columns=score_names)\n",
    "dfAcc = pd.concat([dfAcc, dftmp], ignore_index=True)\n",
    "dfAcc[['TP','FP', 'TN', 'FN']] = dfAcc[['TP','FP', 'TN', 'FN']].astype(int)\n",
    "dfAcc.style.hide_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d50358",
   "metadata": {},
   "source": [
    "# SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e06c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SGD\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "pipe = Pipeline(steps=[('ct', ct), \n",
    "                       ('SGD', SGDClassifier(random_state = 42,\n",
    "                                             class_weight=\"balanced\", \n",
    "                                             warm_start=False, \n",
    "                                             average=False,\n",
    "                                             loss= \"log_loss\",\n",
    "                                           #  learning_rate=\"adaptive\",\n",
    "                                             alpha = 0.05,\n",
    "                                             eta0 = 0.1,\n",
    "                                             learning_rate=\"adaptive\",\n",
    "                                             penalty=\"l1\"\n",
    "                                             \n",
    "                                            )\n",
    "                                            )\n",
    "                          ]\n",
    "                   )\n",
    "\n",
    "param_grid = {#\"SGD__alpha\":[0.01,0.03,0.05], # The higher the value, the stronger the regularization.\n",
    "              #\"SGD__penalty\": [\"l1\",\"l2\", \"elasticnet\"],\n",
    "              #\"SGD__class_weight\": [\"balanced\", 0.5,0.7,1,1.5],\n",
    "              #\"SGD__learning_rate\" : [\"constant\", \"optimal\", \"invscaling\", \"adaptive\"],\n",
    "              #\"SGD__eta0\" : [0.0001,0.001,0.01,0.1]\n",
    "             } \n",
    "\n",
    "\n",
    "# train/validation with the same ratio of classes\n",
    "kfolds = StratifiedKFold(n_splits=4, shuffle=True, random_state=42)\n",
    "\n",
    "grid = GridSearchCV(pipe, \n",
    "                    param_grid = param_grid, \n",
    "                    cv = kfolds, \n",
    "                    scoring = 'f1_macro', \n",
    "                    verbose = 1,\n",
    "                    n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3f3914",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Run the grid search \n",
    "grid.fit(X_train, y_train)\n",
    "print(grid.best_estimator_,'\\n')\n",
    "print('Best parameters  :', grid.best_params_)\n",
    "print('\\nTraining F1 score:', grid.score(X_train, y_train))\n",
    "print(\"Test F1 score:\", grid.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7d03fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probs = grid.predict_proba(X_test)[:,1]\n",
    "scores = print_results('D2_SGD_nt', y_test, grid.predict(X_test), pred_probs)\n",
    "dftmp = pd.DataFrame([scores], columns=score_names)\n",
    "dfAcc = pd.concat([dfAcc, dftmp], ignore_index=True)\n",
    "dfAcc[['TP','FP', 'TN', 'FN']] = dfAcc[['TP','FP', 'TN', 'FN']].astype(int)\n",
    "dfAcc.style.hide_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba2019c",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95106c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "pipe = Pipeline(steps=[('ct', ct),\n",
    "                        ('RF', RandomForestClassifier(random_state = 42, \n",
    "                                                       n_estimators=1000,\n",
    "                                                      criterion=\"gini\", \n",
    "                                                       max_depth=7, \n",
    "                                                       min_samples_split = 30,\n",
    "                                                       max_features='sqrt', \n",
    "                                                     #  min_samples_leaf=10,\n",
    "                                                       class_weight = \"balanced\"\n",
    "                                                     ))\n",
    "                      ]\n",
    "                   )\n",
    "\n",
    "param_grid = { #\"RF__n_estimators\": [750,1000,1500],\n",
    "               # \"RF__max_depth\": [5,6,7],\n",
    "              #  \"RF__min_samples_split\": [45],\n",
    "               # \"RF__max_features\": [\"sqrt\", \"log2\", 10],\n",
    "             # \"RF__criterion\"   : [\"gini\",\"entropy\",\"log_loss\"],\n",
    "                                      } \n",
    "\n",
    "\n",
    "# train/validation with the same ratio of classes\n",
    "kfolds = StratifiedKFold(n_splits=4, shuffle=True, random_state=42)\n",
    "\n",
    "grid = GridSearchCV(pipe, \n",
    "                    param_grid = param_grid, \n",
    "                    cv = kfolds, \n",
    "                    scoring = 'f1_macro', \n",
    "                    verbose = 1,\n",
    "                    n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389e613a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Run the grid search \n",
    "grid.fit(X_train, y_train)\n",
    "print(grid.best_estimator_,'\\n')\n",
    "print('Best parameters  :', grid.best_params_)\n",
    "print('\\nTraining F1 score:', grid.score(X_train, y_train))\n",
    "print(\"Test F1 score:\", grid.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9caedcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probs = grid.predict_proba(X_test)[:,1]\n",
    "scores = print_results('D2_RF_nt', y_test, grid.predict(X_test), pred_probs)\n",
    "dftmp = pd.DataFrame([scores], columns=score_names)\n",
    "dfAcc = pd.concat([dfAcc, dftmp], ignore_index=True)\n",
    "dfAcc[['TP','FP', 'TN', 'FN']] = dfAcc[['TP','FP', 'TN', 'FN']].astype(int)\n",
    "dfAcc.style.hide_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7afe00",
   "metadata": {},
   "source": [
    "# LIGTHGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f912f99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIGHTGBM\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "pipe = Pipeline(steps=[('ct', ct), \n",
    "                       ('LGBM',   lgb.LGBMClassifier(objective=\"binary\",\n",
    "                                                #     class_weight=\"balanced\",                                                   \n",
    "                                                     n_estimators=1500, \n",
    "                                                     learning_rate=0.01,\n",
    "                                                     min_child_weight=25, \n",
    "                                                     max_depth=7, \n",
    "                                                     num_leaves=25, \n",
    "                                                     min_child_samples=50,                                                      \n",
    "                                                     reg_alpha=0.8, \n",
    "                                                     reg_lambda=0.8,\n",
    "                                                 #    subsample_freq=0, \n",
    "                                                     colsample_bytree=0.3, \n",
    "                                                     subsample=0.8,\n",
    "                                                     min_split_gain = 25,\n",
    "                                                     min_data_in_leaf = 25,\n",
    "                                                     random_state=42))\n",
    "                          ]\n",
    "                   )\n",
    "\n",
    "param_grid = {\n",
    "              #   'LGBM__n_estimators': [750,1000,1500], \n",
    "              #   'LGBM__learning_rate' : [0.0001,0.001,0.01],\n",
    "              #    'LGBM__min_child_weight': [250], \n",
    "              #    'LGBM__max_depth' : [5], \n",
    "               #  'LGBM__num_leaves': [250], \n",
    "              #   'LGBM__min_child_samples': [100,150,200,250,300],                                                      \n",
    "              #   'LGBM__reg_alpha' : [5,6,7], # default 0 \n",
    "              #  'LGBM__min_data_in_leaf' : [100,150,200,250,300],\n",
    "              #   'LGBM__reg_lambda' : [1,1.5,2,2.5,3], # default 0 \n",
    "              #   'LGBM__subsample_freq' : [1,5,10,100,500], \n",
    "             #  'LGBM__subsample' : [0.5,0.6,0.7,0.8,0.9], \n",
    "              # 'LGBM__colsample_bytree' : [0.5,0.6,0.7,0.8,0.9], \n",
    "              #  'LGBM__min_split_gain' : [122], \n",
    "             } \n",
    "\n",
    "\n",
    "# train/validation with the same ratio of classes\n",
    "kfolds = StratifiedKFold(n_splits=4, shuffle=True, random_state=42)\n",
    "\n",
    "grid = GridSearchCV(pipe, \n",
    "                    param_grid = param_grid, \n",
    "                    cv = kfolds, \n",
    "                    scoring = 'f1_macro', \n",
    "                    verbose = 2,\n",
    "                    n_jobs = -1)\n",
    "# default lgbm = 0.57"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b161b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Run the grid search \n",
    "grid.fit(X_train, y_train)\n",
    "print(grid.best_estimator_,'\\n')\n",
    "print('Best parameters  :', grid.best_params_)\n",
    "print('\\nTraining F1 score:', grid.score(X_train, y_train))\n",
    "print(\"Test F1 score:\", grid.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574bfac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probs = grid.predict_proba(X_test)[:,1]\n",
    "scores = print_results('D2_LGBM_nt', y_test, grid.predict(X_test), pred_probs)\n",
    "dftmp = pd.DataFrame([scores], columns=score_names)\n",
    "dfAcc = pd.concat([dfAcc, dftmp], ignore_index=True)\n",
    "dfAcc[['TP','FP', 'TN', 'FN']] = dfAcc[['TP','FP', 'TN', 'FN']].astype(int)\n",
    "dfAcc.style.hide_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f66036c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAcc_pck = dfAcc.copy()\n",
    "import pickle \n",
    "fd = open(\"OM_D2_results_table\", 'wb') \n",
    "pickle.dump(dfAcc_pck, fd)\n",
    "fd.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
