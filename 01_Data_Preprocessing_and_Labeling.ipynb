{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc646f75",
   "metadata": {},
   "source": [
    "# Data cleaning, preprocessing and labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a694f0",
   "metadata": {},
   "source": [
    "**Author:** Bilge Nur Karaca\n",
    "\n",
    "*Feature names used in this project are altered and does not perfectly represent the original feature names.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4b8d2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ad56ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data\n",
    "data_df = pd.read_csv(\"private_dataset.txt\", encoding='unicode_escape', sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf8e730",
   "metadata": {},
   "source": [
    "## Filtering data according to pre-defined time periods ruleset\n",
    "\n",
    "In our dataset, the churn status of customers is absent, so our data is unlabeled. \n",
    "\n",
    "\n",
    "The main reason behind that is the non-contractual nature of customer relationship e-commerce context. As vague is the moment at which a customer quits purchasing from a brand, it can be only detected through creation of a ruleset. \n",
    "\n",
    "For our project, we will thereby define churn event ourselves. In doing so, we take into consideration the business setting with utmost attention such as consumption cycle of the products, number of repeat purchase per shopper etc.\n",
    "\n",
    "After careful examination for our dataset, we use the following periods: \n",
    "* 9-months prediction audience pool **and** observation,\n",
    "* 6-months **only** observation period, \n",
    "* 6-months churn prediction.\n",
    "\n",
    " \n",
    "Put differently, we take **the customers that have made at least one purchase starting from 15 months ago to 6 months ago**, and we predict their churn situation for **the upcoming 6 months**. \n",
    "\n",
    "**We are not interested in the customers that have made a purchase since last 6 months, because their purchase is too recent (i.e. they are active).**\n",
    "\n",
    "As this ruleset is crucial to **avoid data leakage**, we created relevant variables right away. See below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0091f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train data FEATURE period\n",
    "\n",
    "train_data_feature_period_start_date = pd.to_datetime(\"2021-03-01 00:00:00\")\n",
    "train_data_feature_period_end_date = pd.to_datetime(\"2022-06-01 00:00:00\") \n",
    "\n",
    "# Train data CONTROL period\n",
    "\n",
    "train_data_control_period_start_date = pd.to_datetime(\"2022-06-01 00:00:00\")\n",
    "train_data_control_period_end_date = pd.to_datetime(\"2022-12-01 00:00:00\") #\n",
    "\n",
    "# --------------------------------------------------------\n",
    "\n",
    "# TEST data FEATURE period\n",
    "\n",
    "test_data_feature_period_start_date = pd.to_datetime(\"2021-09-01 00:00:00\") \n",
    "test_data_feature_period_end_date = pd.to_datetime(\"2022-12-01 00:00:00\")\n",
    "\n",
    "# TEST data CONTROL period\n",
    "\n",
    "test_data_control_period_start_date = pd.to_datetime(\"2022-12-01 00:00:00\")\n",
    "test_data_control_period_end_date = pd.to_datetime(\"2023-06-01 00:00:00\")\n",
    "\n",
    "# --------------------------------------------------------\n",
    "\n",
    "train_dataset_global_last_day = \"2023-03-01 00:00:00\"\n",
    "test_dataset_global_last_day = \"2023-06-01 00:00:00\"\n",
    "\n",
    "# --------------------------------------------------------\n",
    "\n",
    "# Train data customer-at-risk period\n",
    "\n",
    "observation_starts = pd.to_datetime('2021-03-01 00:00:00')\n",
    "observation_ends = pd.to_datetime('2021-12-01 00:00:00')\n",
    "\n",
    "\n",
    "# Test data customer-at-risk period\n",
    "\n",
    "observation_starts_test = pd.to_datetime('2021-09-01 00:00:00')\n",
    "observation_ends_test = pd.to_datetime('2022-06-01 00:00:00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4953285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preserve the initial omnichannel dataset in a variable. The rest is performed on online channel.\n",
    "\n",
    "omni_data = data_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75e15af",
   "metadata": {},
   "source": [
    "## Fix the glitches in the dataset & detect aggregation needs to achieve the desired dataset format (based on CustomerID index)\n",
    "\n",
    "1. Filter online channel.\n",
    "2. Columns names are skipped by 1. Drop 'X_status' from columns names and ve add \"to-be-removed\"(tbr) column at the end.\n",
    "3. Remove \"tbr\" column. Drop columns which have only 1 unique value.\n",
    "4. Detected 2 test users. Remove 2 test users & their corresponding rows.\n",
    "5. Create dataframes separately for customer ve transaction features.\n",
    "6. Group the customer dataframe by aggregating fields using \"last\" function.\n",
    "7. Group the transaction dataframe by aggregating fields.\n",
    "8. Replace the Turkish characters in location names.\n",
    "9. Replace misspellings in the City values.\n",
    "10. Examine net amount or price == 0 cases.\n",
    "11. Remove col7 ve col8 due to high % of null values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fb843f",
   "metadata": {},
   "source": [
    "### 1. Filter online channel data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5632c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter online\n",
    "\n",
    "data_df = data_df[data_df.ChannelName == \"ONLINE CHANNEL\"]\n",
    "\n",
    "print(data_df[[\"ChannelName\"]].value_counts())\n",
    "print(data_df.shape[0])\n",
    "print(data_df.isna().sum()/data_df.shape[0])\n",
    "data_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca00d5e9",
   "metadata": {},
   "source": [
    "### 2. Columns names are skipped by 1. Drop 'X_status' from columns names and add \"to-be-removed\"(tbr) column at the end.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aea135b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cols = [\"Insert\", \"the\",\"list\", \"of\", \"column\", \"names\", \"here\"]\n",
    "data_df.columns = new_cols\n",
    "data_df.columns "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50a2c07",
   "metadata": {},
   "source": [
    "### 3. Remove \"tbr\" column. Drop columns which have only 1 unique value. Remove 'ProductID','Phone', 'Email'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e96afe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove columns that have only 1 unique value or all null\n",
    "\n",
    "unique_val_cols = []\n",
    "for col in data_df.columns:\n",
    "    if len(data_df[col].unique()) <= 1: \n",
    "        print(col)\n",
    "        unique_val_cols.append(col)\n",
    "print(unique_val_cols)\n",
    "\n",
    "data_df.drop(unique_val_cols, axis=1,inplace=True)\n",
    "\n",
    "# Drop columns that does not have value: - Phone and Email unique // col7 and col8 are %80 null. \n",
    "\n",
    "data_df.drop(['ProductID','Phone', 'Email', 'Email2', 'col7', 'col8'], axis=1,inplace=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baaad169",
   "metadata": {},
   "source": [
    "### 4. Investigate suspicious users. Remove 2 test users & their rows.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a20aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking unique city value counts. Detecting more unique city names than those which exist.\n",
    "\n",
    "data_df[[\"CustomerID\", \"City\"]].groupby(\"CustomerID\").nunique().sort_values(by=\"City\")\n",
    "\n",
    "# Checking the user w/ impossible number of unique cities.\n",
    "# TEST USER 1\n",
    "\n",
    "print(data_df[\"City\"][data_df[\"CustomerID\"]== \"{HIDDEN-CUSTOMER-ID}\"].unique())\n",
    "\n",
    "# TEST USER 2: too many cities.  \n",
    "\n",
    "data_df[\"City\"][data_df[\"CustomerID\"]== \"{HIDDEN-CUSTOMER-ID2}\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dc862f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# more investigation \n",
    "\n",
    "test_user1 = \"{HIDDEN-CUSTOMER-ID}\"\n",
    "data_df[[\"CustomerID\",\"City\",\"NetAmount\"]][data_df[\"CustomerID\"] == test_user1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f08361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# more investigation \n",
    "\n",
    "test_user2 = \"{HIDDEN-CUSTOMER-ID2}\"\n",
    "data_df[[\"CustomerID\",\"City\",\"NetAmount\",\"Price\"]][data_df[\"CustomerID\"] == \n",
    "                                                   test_user2].sort_values(by=\"Price\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aed17f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# more investigation \n",
    "\n",
    "print(\"Raws inc. test_user_1:\", len(list(data_df[\"CustomerID\"][(data_df[\"CustomerID\"]==test_user1)])))\n",
    "print(\"Raws inc. test_user_2:\",len(list(data_df[\"CustomerID\"][(data_df[\"CustomerID\"]==test_user2)])))\n",
    "\n",
    "test_user_idx = list(data_df[\"CustomerID\"][(data_df[\"CustomerID\"]==test_user1)\n",
    "                                           |(data_df[\"CustomerID\"]==test_user2)]\n",
    "                     .index)\n",
    "print(\"Raws inc. all test users:\",len(test_user_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ff6993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop test users\n",
    "\n",
    "data_df.drop(test_user_idx, axis=0, inplace=True)\n",
    "data_df.shape\n",
    "\n",
    "# Reset index & drop index column\n",
    "\n",
    "data_df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bab0070",
   "metadata": {},
   "source": [
    "### 5. Replace the Turkish characters in location names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbd027a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turkish char adjustments\n",
    "\n",
    "data_df['col1'] = data_df['col1'].str.replace(\"Ý\",\"I\").str.replace(\"ý\",\"i\").str.replace(\"ð\",\"g\").str.replace(\"þ\",\"s\").str.replace(\"Þ\",\"S\")\n",
    "data_df['col2'] = data_df['col2'].str.replace(\"Ý\",\"I\").str.replace(\"ý\",\"i\").str.replace(\"ð\",\"g\").str.replace(\"þ\",\"s\").str.replace(\"Þ\",\"S\")\n",
    "data_df['col3'] = data_df['col3'].str.replace(\"Ý\",\"I\").str.replace(\"ý\",\"i\").str.replace(\"ð\",\"g\").str.replace(\"þ\",\"s\").str.replace(\"Þ\",\"S\")\n",
    "data_df['col4'] = data_df['col4'].str.replace(\"Ý\",\"I\").str.replace(\"ý\",\"i\").str.replace(\"ð\",\"g\").str.replace(\"þ\",\"s\").str.replace(\"Þ\",\"S\")\n",
    "data_df['col5'] = data_df['col5'].str.replace(\"Ý\",\"I\").str.replace(\"ý\",\"i\").str.replace(\"ð\",\"g\").str.replace(\"þ\",\"s\").str.replace(\"Þ\",\"S\")\n",
    "data_df['col5'] = data_df['col6'].str.replace(\"Ý\",\"I\").str.replace(\"ý\",\"i\").str.replace(\"ð\",\"g\").str.replace(\"þ\",\"s\").str.replace(\"Þ\",\"S\")\n",
    "\n",
    "# City name adjustments\n",
    "\n",
    "data_df[[\"col1\",\"col2\",\"col3\",\"col4\",\"col5\",\"col6\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d124546",
   "metadata": {},
   "source": [
    "### 6. Replace null values of CouponType as \"no_coupon\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adbb9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df[\"CouponType\"].fillna(\"no_coupon_used\",inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3bca70",
   "metadata": {},
   "source": [
    "### 7. Remove missing values of City1 by filling from City2 as long as District1 matches District2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16da71bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill out missing values of City1 by City2 if districts of both match.\n",
    "\n",
    "pd.options.display.max_columns=40\n",
    "\n",
    "data_df[\"City1\"] = np.where((data_df[\"City2\"]!= data_df[\"City1\"]) & \n",
    "                                     (data_df[\"District2\"] != data_df[\"Distric1\"]) &\n",
    "                                     (data_df[\"City1\"].apply(str).apply(len) > 15),\n",
    "                                        \"flag\",data_df[\"City2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0abd9b",
   "metadata": {},
   "source": [
    "### 8. Examine products with Price == 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2bd325",
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_with_0_idx = list(data_df[[\"CustomerID\"]][data_df[\"Price\"] == 0].index)\n",
    "cust_with_0_custID = list(data_df[\"CustomerID\"].iloc[cust_with_0_idx].unique())\n",
    "\n",
    "cust_with_0_agg = data_df[[\"CustomerID\",\"DiscountAmount\",\"NetAmount\"]][(data_df[\"CustomerID\"]\n",
    "                                                .isin(cust_with_0_custID))].groupby(\"CustomerID\").sum().sort_values(by=\"NetAmount\")\n",
    "\n",
    "cust_with_0_agg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37d6098",
   "metadata": {},
   "source": [
    "### 9. Datetime conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5136b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df[\"BirthDate\"] = pd.to_datetime(data_df[\"BirthDate\"], format=\"%Y-%m-%d %H:%M:%S\" )\n",
    "data_df[\"TransactionDate\"] = pd.to_datetime(data_df[\"TransactionDate\"], format=\"%Y-%m-%d %H:%M:%S\" )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d627a6",
   "metadata": {},
   "source": [
    "### 10. Create # of days features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba576d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to get number of days since the input date\n",
    "\n",
    "# global_last_day = \"2023-06-17 15:00:00\"\n",
    "\n",
    "def days_since(last_tranaction_date_of_customer, dataset_global_last_day):\n",
    "    dataset_global_last = pd.to_datetime(dataset_global_last_day, format=\"%Y-%m-%d %H:%M:%S\" )\n",
    "    days_since= dataset_global_last - last_tranaction_date_of_customer\n",
    "    days_since =days_since.days\n",
    "    return days_since\n",
    "\n",
    "data_df[\"TransactionDate\"].apply(days_since)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76708a4",
   "metadata": {},
   "source": [
    "### 11. Add an offline-channel transaction number feature\n",
    "\n",
    "This feature will enable the model to learn offline behavior of the customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb4951d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get unique transactions per customer\n",
    "\n",
    "omni_data = omni_data.drop_duplicates(subset = [\"CustomerID\", \"TransactionDate\"])\n",
    "\n",
    "# datetime conversion\n",
    "\n",
    "omni_data[\"TransactionDate\"] = pd.to_datetime(omni_data[\"TransactionDate\"], format=\"%Y-%m-%d %H:%M:%S\" )\n",
    "\n",
    "# Filter according to Train data FEATURE period\n",
    "\n",
    "omni_data_feature_period_train = omni_data[(omni_data[\"TransactionDate\"]> train_data_feature_period_start_date) &\n",
    "                                                      (omni_data[\"TransactionDate\"]< train_data_feature_period_end_date)].reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Filter according to TEST data FEATURE period\n",
    "\n",
    "\n",
    "omni_data_feature_period_test = omni_data[(omni_data[\"TransactionDate\"]> test_data_feature_period_start_date) &\n",
    "                                                      (omni_data[\"TransactionDate\"]< test_data_feature_period_end_date)].reset_index(drop=True)\n",
    "\n",
    "# Group by Customer ID to get transaction number\n",
    "\n",
    "omnichannel_trx_num_train = omni_data_feature_period_train[[\"CustomerID\", \n",
    "                                                            \"TransactionDate\"]].groupby(\"CustomerID\").count()\n",
    "\n",
    "omnichannel_trx_num_test = omni_data_feature_period_test[[\"CustomerID\", \n",
    "                                                            \"TransactionDate\"]].groupby(\"CustomerID\").count()\n",
    "\n",
    "# Get online transaction number\n",
    "\n",
    "online_trx_num_train = omni_data_feature_period_train[[\"CustomerID\", \n",
    "                                                         \"TransactionDate\"\n",
    "                                                        ]\n",
    "                               ][omni_data_feature_period_train.ChannelName == \"ONLINE CHANNEL\"].groupby(\"CustomerID\").count()\n",
    "\n",
    "online_trx_num_test = omni_data_feature_period_test[[\"CustomerID\", \n",
    "                                                     \"TransactionDate\"\n",
    "                                                    ]\n",
    "                               ][omni_data_feature_period_test.ChannelName == \"ONLINE CHANNEL\"].groupby(\"CustomerID\").count()\n",
    "\n",
    "\n",
    "# Merge online & omnichannel purchase number \n",
    "\n",
    "trx_merged_train = online_trx_num_train.merge(omnichannel_trx_num_train, how=\"left\", on=\"CustomerID\")\n",
    "trx_merged_test  = online_trx_num_test.merge(omnichannel_trx_num_test, how=\"left\", on=\"CustomerID\")\n",
    "\n",
    "# Get offline number by substraction\n",
    "\n",
    "trx_merged_train[\"offline\"] = trx_merged_train[\"TransactionDate_y\"] - trx_merged_train[\"TransactionDate_x\"]\n",
    "trx_merged_test[\"offline\"] = trx_merged_test[\"TransactionDate_y\"] - trx_merged_test[\"TransactionDate_x\"]\n",
    "\n",
    "\n",
    "offline_purchase_train = trx_merged_train[[\"offline\"]].reset_index()\n",
    "offline_purchase_test = trx_merged_test[[\"offline\"]].reset_index()\n",
    "\n",
    "# Add these to the dataframe\n",
    "\n",
    "offline_purchase_train\n",
    "offline_purchase_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "004a99e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "457 days 00:00:00\n",
      "183 days 00:00:00\n"
     ]
    }
   ],
   "source": [
    "train_feature_period_length = print((train_data_feature_period_end_date-train_data_feature_period_start_date))\n",
    "train_control_period_length = print((train_data_control_period_end_date-train_data_control_period_start_date))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e366dc1",
   "metadata": {},
   "source": [
    "## 12. Create customer-based and order-based dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cece0d90",
   "metadata": {},
   "source": [
    "## SPLIT DATA FOR TRAIN & TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf45af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the transaction data according to dates\n",
    "\n",
    "train_df = data_df[(data_df[\"TransactionDate\"]> train_data_feature_period_start_date) &\n",
    "                    (data_df[\"TransactionDate\"]< train_data_feature_period_end_date)].reset_index(drop=True) #sadece featurelar\n",
    "\n",
    "\n",
    "# --------------------------------------------------------\n",
    "\n",
    "test_df = data_df[(data_df[\"TransactionDate\"]> test_data_feature_period_start_date) &\n",
    "                    (data_df[\"TransactionDate\"]< test_data_feature_period_end_date)].reset_index(drop=True)\n",
    "\n",
    "print(train_data_feature_period_end_date-train_data_feature_period_start_date)\n",
    "print(test_data_feature_period_end_date-test_data_feature_period_start_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b73894f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For TRAIN data, slice the transaction date of the observation (pooling) period\n",
    "\n",
    "df_train_only_for_observation_period = data_df[(data_df[\"TransactionDate\"]> observation_starts) &\n",
    "                                                      (data_df[\"TransactionDate\"]< observation_ends)].reset_index(drop=True)\n",
    "\n",
    "# We only look for which customers has made a purchase within this period.\n",
    "# Get labels for that.\n",
    "\n",
    "observation_labels = df_train_only_for_observation_period.groupby(\"CustomerID\").sum()\n",
    "observation_labels[\"label\"] = 1\n",
    "observation_labels.drop([\"TransactionDate_code\",\n",
    "                         \"Price\",\n",
    "                         \"Quantity\",\n",
    "                         \"Amount\",\n",
    "                         \"Discount_Amount\",\n",
    "                         \"Net_Amount\",\n",
    "                         \"Basket_Size\"], axis=1, inplace=True)\n",
    "observation_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed7bd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For TEST data, slice the transaction data of the observation (pooling) period\n",
    "\n",
    "df_test_only_for_observation_period = data_df[(data_df[\"TransactionDate\"]> observation_starts_test) &\n",
    "                                                      (data_df[\"TransactionDate\"]< observation_ends_test)].reset_index(drop=True)\n",
    "\n",
    "# We only look for which customers has made a purchase within this period.\n",
    "# Get labels for that\n",
    "\n",
    "observation_labels_test = df_test_only_for_observation_period.groupby(\"CustomerID\").sum()\n",
    "observation_labels_test[\"label\"] = 1\n",
    "observation_labels_test.drop([\"TransactionDate_code\",\n",
    "                         \"Price\",\n",
    "                         \"Quantity\",\n",
    "                         \"Amount\",\n",
    "                         \"Discount_Amount\",\n",
    "                         \"Net_Amount\",\n",
    "                         \"Basket_Size\"], axis=1, inplace=True)\n",
    "observation_labels_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acad1c1",
   "metadata": {},
   "source": [
    "### CREATE ORDER-BASED-FEATURES DATAFRAME / Unique identifier is InvoiceID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe21577f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put together values that occur per purchase and that stay same within the detail lines of a given pruchse\n",
    "\n",
    "def func_transform1(data_df):\n",
    "    bask_data_part1=  data_df[[\"CustomerID\",\n",
    "                                \"InvoiceID\",\n",
    "                                \"Invoice_Type\",\n",
    "                                \"TransactionDate\",\n",
    "                                \"City2\",\n",
    "                                \"CampaignCouponCode\",\n",
    "                                \"City1\",\n",
    "                                \"Device\",\n",
    "                                #\"Discount_Amount\",\n",
    "                                #\"Net_Amount\",\n",
    "                                \"Basket_Size\"]].drop_duplicates(subset=[\"InvoiceID\"])\n",
    "    return bask_data_part1\n",
    "\n",
    "# aggregate values per purchase. Those are features that change across different lines of a pruchase.\n",
    "\n",
    "def func_transform2(data_df):\n",
    "    bask_data_part2 = data_df[[ \"InvoiceID\",\n",
    "                                \"Discount_Amount\",\n",
    "                                \"Net_Amount\"]].groupby(\"InvoiceID\").agg({'Discount_Amount': ['sum'],\n",
    "                                                                        'Net_Amount': ['sum']})\n",
    "    return bask_data_part2\n",
    "\n",
    "# merge two subsets of basket_data\n",
    "\n",
    "def func_transform3(bask_data_part1, bask_data_part2):\n",
    "    bask_data = bask_data_part1.merge(bask_data_part2, on=\"InvoiceID\", how=\"outer\")\n",
    "    return bask_data\n",
    "\n",
    "# create a bool column to see whether there's a change between two addresses\n",
    "\n",
    "def func_transform4(bask_data):\n",
    "    bask_data[\"isShippedToBilled\"] = np.where(bask_data[\"City1\"] == bask_data[\"City2\"],1,0)\n",
    "    return\n",
    "\n",
    "# drop city1 columns\n",
    "\n",
    "def func_transform5(bask_data):\n",
    "    bask_data.drop(\"City1\",axis=1,inplace=True)\n",
    "    return\n",
    "\n",
    "# calculate days since the invoice is processed\n",
    "\n",
    "def func_transform6(bask_data, dataset_global_last_d):\n",
    "    bask_data[\"days_since_transaction_occurred\"] = bask_data[\"TransactionDate\"].apply(lambda x: days_since(x, dataset_global_last_d))                                                                                             \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eceecd1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# APPLY TRANSFORMATION FUNCTIONS\n",
    "\n",
    "# Put together values that occur per purchase and that stay same within the detail lines of a given pruchse\n",
    "bask_data_part1_train = func_transform1(train_df)\n",
    "bask_data_part1_test = func_transform1(test_df)\n",
    "\n",
    "# Aggregate values per purchase. Those are features that change across different lines of a pruchase.\n",
    "bask_data_part2_train = func_transform2(train_df)\n",
    "bask_data_part2_test = func_transform2(test_df)\n",
    "\n",
    "# Merge two subsets of bask_data\n",
    "bask_data_train = func_transform3(bask_data_part1_train, bask_data_part2_train)\n",
    "bask_data_test = func_transform3(bask_data_part1_test, bask_data_part2_test)\n",
    "\n",
    "# Create a bool column to see whether there's a change between billing and shipping address\n",
    "func_transform4(bask_data_train)\n",
    "func_transform4(bask_data_test)\n",
    "\n",
    "# Drop shipping city\n",
    "func_transform5(bask_data_train)\n",
    "func_transform5(bask_data_test)\n",
    "\n",
    "# Calculate days since the invoice is processed\n",
    "func_transform6(bask_data_train, train_dataset_global_last_day)\n",
    "func_transform6(bask_data_test, test_dataset_global_last_day)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68afe7a1",
   "metadata": {},
   "source": [
    "### CREATE CUSTOMER-BASED-FEATURES DATAFRAME / Unique identifier is CustomerID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9245ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_features = ['CustomerID', 'BirthDate','isContactable']\n",
    "\n",
    "\n",
    "def func_transform7(data_df, cust_features):\n",
    "    cust_data = pd.pivot_table(data=data_df, \n",
    "                               index=data_df[\"CustomerID\"],\n",
    "                               values = data_df[cust_features],\n",
    "                               aggfunc='last') # last value is the most updated\n",
    "    cust_data.reset_index(drop=True,inplace=True)\n",
    "    return cust_data\n",
    "\n",
    "cust_data_train = func_transform7(train_df, cust_features)\n",
    "cust_data_test = func_transform7(test_df, cust_features)\n",
    "\n",
    "cust_data_train.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7815e410",
   "metadata": {},
   "source": [
    "### JOIN 1: CUSTOMER-BASED-DATA + ORDER-BASED-DATA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57458259",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_transform8(cust_data, bask_data):\n",
    "    bask_cust_data = cust_data.merge(bask_data, on=\"CustomerID\", how=\"outer\")\n",
    "    \n",
    "    bask_cust_data.columns = [\"insert\", \"column\", \"names\", \"here\"]\n",
    "    \n",
    "    return bask_cust_data\n",
    "\n",
    "bask_cust_data_train = func_transform8(cust_data_train, bask_data_train)\n",
    "bask_cust_data_test = func_transform8(cust_data_test, bask_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ee84b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bask_cust_data_train[bask_cust_data_train[\"CustomerID\"].isna()==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61109735",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_transform9(bask_cust_data):\n",
    "    \n",
    "    df =    bask_cust_data[\n",
    "                [\n",
    "                    \"CustomerID\",\n",
    "                    \"BirthDate\",\n",
    "                    \"isContactable\",\n",
    "                    \"City\",\n",
    "                    \"InvoiceID\",\n",
    "                    \"CouponType\",\n",
    "                    \"Device\",\n",
    "                    \"Basket_Size\",\n",
    "                    \"Discount_Amount_sum\",\n",
    "                    \"Net_Amount_sum\",\n",
    "                    'isShippedToBilled',\n",
    "                    'days_since_transaction_occured']].groupby(\"CustomerID\").agg(\n",
    "                {\n",
    "                    'BirthDate': ['last'], \n",
    "                    'isContactable': ['last'], \n",
    "                    'City':['last'], \n",
    "                    'InvoiceID':['count'], # total number of orders\n",
    "                    'CouponType': ['last'], #total times a coupon is used\n",
    "                    'Basket_Size':[np.mean, sum, 'min', 'max'], # \n",
    "                    \"Discount_Amount_sum\": [np.mean, sum,'count'], # how many times a discount is used etc.\n",
    "                    \"Net_Amount_sum\": [np.mean, sum], #AOV, total moneraty value\n",
    "                    'isShippedToBilled': [sum],\n",
    "                    'days_since_transaction_occured': ['min', 'max'] # days since first & last order\n",
    "                })\n",
    "\n",
    "\n",
    "    df.columns = ['BirthDate',\n",
    "                   'isContactable',\n",
    "                    'City_last',\n",
    "                    'InvoiceID_count',\n",
    "                    'CouponType_last',\n",
    "                    'Basket_Size_mean',\n",
    "                    'Basket_Size_sum',\n",
    "                    'Basket_Size_min',\n",
    "                    'Basket_Size_max',\n",
    "                    'Discount_Amount_sum_mean',\n",
    "                    'Discount_Amount_sum_sum',\n",
    "                    'Discount_Amount_sum_count',\n",
    "                    'Net_Amount_sum_mean',\n",
    "                    'Net_Amount_sum_sum',\n",
    "                    'isShippedToBilled_sum',\n",
    "                    'days_since_last_purchase',\n",
    "                    'days_since_first_purchase']  \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c3f674",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = func_transform9(bask_cust_data_train)\n",
    "df_test = func_transform9(bask_cust_data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642d7067",
   "metadata": {},
   "source": [
    "#### Add \"number_of_returns\" feature to df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adb4b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_transform10(bask_data, df):\n",
    "\n",
    "    # get number of returns by aggregation\n",
    "    num_returns = bask_data[[\"CustomerID\",\"Invoice_Type\"]][bask_data[\"Invoice_Type\"]==\"Return\"].groupby(\"CustomerID\").count()\n",
    "\n",
    "    # merge number of returns column with df\n",
    "    df = df.merge(num_returns, on =\"CustomerID\", how=\"left\")\n",
    "\n",
    "    # fill na with 0 (since no returns happened)\n",
    "    df[\"Invoice_Type\"].fillna(0,inplace=True)\n",
    "\n",
    "    # rename the column\n",
    "    df.rename(columns={\"Invoice_Type\": \"num_returns\"},inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "df_train = func_transform10(bask_data_train, df_train)\n",
    "df_test = func_transform10(bask_data_test, df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba7a859",
   "metadata": {},
   "source": [
    "#### Add device_type to df (first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae80ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.options.display.min_rows=50\n",
    "\n",
    "def func_transform11(bask_data, df):\n",
    "    device_most_recent = bask_data.sort_values(by=\n",
    "                                                 \"days_since_transaction_occurred\")[[\"CustomerID\", \n",
    "                                                                                     \"Device\"\n",
    "                                                                                    ]\n",
    "                                                                                ].groupby(\"CustomerID\").agg(\n",
    "                                                                                    {\n",
    "                                                                                     \"Device\": 'first'\n",
    "                                                                                    }\n",
    "    )\n",
    "\n",
    "\n",
    "    df = df.merge(device_most_recent,on=\"CustomerID\", how=\"left\")\n",
    "    return df\n",
    "\n",
    "df_train = func_transform11(bask_data_train, df_train)\n",
    "df_test = func_transform11(bask_data_test, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c5b2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_transform11(data_df, df):\n",
    "    \n",
    "    # Compute frequencies for each axe per customer (how many products from each axe)\n",
    "\n",
    "    ct_axe = pd.crosstab(data_df[\"CustomerID\"], data_df[\"Product_Category\"])\n",
    "\n",
    "    # Merge with the dataset\n",
    "\n",
    "    df = df.merge(ct_axe,how=\"left\",on=\"CustomerID\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "#---------------------\n",
    "\n",
    "def func_transform12(data_df, df):\n",
    "    # Compute frequencies for each axe per customer (how many products from each axe)\n",
    "\n",
    "    ct_grp = pd.crosstab(data_df[\"CustomerID\"], data_df[\"Product_Axe\"])\n",
    "\n",
    "    # Merge with the dataset\n",
    "\n",
    "    df= df.merge(ct_grp,how=\"left\",on=\"CustomerID\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "df_train = func_transform11(train_df, df_train)\n",
    "df_test = func_transform11(test_df, df_test)\n",
    "\n",
    "df_train = func_transform12(train_df, df_train)\n",
    "df_test = func_transform12(test_df, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319b4d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_recent_return = bask_data[[\"CustomerID\",\n",
    "                                \"TransactionDate\"]\n",
    "                               ][bask_data[\"Invoice_Type\"]==\"Return\"].groupby(\"CustomerID\").max()\n",
    "\n",
    "most_recent_return[\"days_since_last_return\"] = most_recent_return[\"TransactionDate\"].apply(days_since_last_purchase)\n",
    "most_recent_return.drop(\"TransactionDate\", axis=1, inplace=True)\n",
    "most_recent_return[\"days_since_last_return\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0eee2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_transform13(df, dataset_global_last_d):\n",
    "    df[\"Age\"] = df[\"BirthDate\"].apply(lambda x: days_since(x, dataset_global_last_d))\n",
    "    df[\"Age\"] = df[\"Age\"] / 365\n",
    "    df.drop(\"BirthDate\",inplace=True, axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "df_train = func_transform13(df_train, train_dataset_global_last_day)\n",
    "df_test = func_transform13(df_test, test_dataset_global_last_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e000a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add offline\n",
    "\n",
    "df_train = df_train.merge(offline_purchase_train,on=\"CustomerID\",how=\"left\")\n",
    "df_test = df_test.merge(offline_purchase_test,on=\"CustomerID\",how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca0d263",
   "metadata": {},
   "source": [
    "# Train Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac30868f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a transaction-based dataframe that covers only the control period\n",
    "\n",
    "control_group_df = data_df[(data_df[\"TransactionDate\"] >= pd.to_datetime(train_data_control_period_start_date)) & \n",
    "                        (data_df[\"TransactionDate\"] < pd.to_datetime(train_data_control_period_end_date))]\n",
    "\n",
    "\n",
    "# label those as \"not a churn\"\n",
    "\n",
    "control_group_df[\"label\"]=0\n",
    "\n",
    "# take only necessary columns\n",
    "\n",
    "control_group_df = control_group_df[[\"CustomerID\",\"label\"]]\n",
    "\n",
    "# unify customers\n",
    "\n",
    "control_group_df = control_group_df.groupby(\"CustomerID\").sum()\n",
    "\n",
    "# Rename the \"label\" column in distinctive way\n",
    "\n",
    "observation_labels[\"observation_label\"] = 1\n",
    "observation_labels.drop([\"label\"],axis=1,inplace=True)\n",
    "\n",
    "# Merge control group and observation. We want all the observation list.\n",
    "\n",
    "label_prep = observation_labels.merge(control_group_df, on=\"CustomerID\", how=\"left\")\n",
    "label_prep.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002a0f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_prep.drop(\"observation_label\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fd7538",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(control_group_df.shape)\n",
    "print(observation_labels.shape)\n",
    "print(train_data_control_period_end_date-train_data_control_period_start_date)\n",
    "print(test_data_control_period_end_date-test_data_control_period_start_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4717da7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_prep.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71441ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_prep[\"label\"].fillna(1, inplace=True)\n",
    "label_prep.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d1102a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train.shape)\n",
    "print(df_test.shape)\n",
    "print(label_prep.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d171f2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = df_train.merge(label_prep, on=\"CustomerID\", how=\"left\")\n",
    "\n",
    "# only customers that we selected have labels. To drop other customers, remove when label = null\n",
    "\n",
    "train_data = train_data[train_data[\"label\"].isna()==False]\n",
    "\n",
    "print(\"Labeling is done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f62b876",
   "metadata": {},
   "source": [
    "### Test labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebbd686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a transaction-based dataframe that covers only the control period\n",
    "\n",
    "control_group_df_test = data_df[(data_df[\"TransactionDate\"] >= pd.to_datetime(test_data_control_period_start_date)) & \n",
    "                        (data_df[\"TransactionDate\"] < pd.to_datetime(test_data_control_period_end_date))]\n",
    "\n",
    "\n",
    "# label those as \"not a churn\"\n",
    "\n",
    "control_group_df_test[\"label\"]=0\n",
    "\n",
    "# take only necessary columns\n",
    "\n",
    "control_group_df_test = control_group_df_test[[\"CustomerID\",\"label\"]]\n",
    "\n",
    "# unify customers\n",
    "\n",
    "control_group_df_test = control_group_df_test.groupby(\"CustomerID\").sum()\n",
    "\n",
    "# Rename the \"label\" column in distinctive way\n",
    "\n",
    "observation_labels_test[\"observation_label\"] = 1\n",
    "observation_labels_test.drop([\"label\"],axis=1,inplace=True)\n",
    "\n",
    "# Merge control group and observation. We want all the observation list.\n",
    "\n",
    "label_prep_test = observation_labels_test.merge(control_group_df_test, on=\"CustomerID\", how=\"left\")\n",
    "label_prep_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74521e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_prep_test.drop(\"observation_label\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7d972e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(control_group_df_test.shape)\n",
    "print(observation_labels_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db837efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_prep_test[\"label\"].fillna(1, inplace=True)\n",
    "label_prep_test.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9dc6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge labels & features\n",
    "\n",
    "test_data = df_test.merge(label_prep_test, on=\"CustomerID\", how=\"left\")\n",
    "\n",
    "# only customers that we selected have labels. To drop other customers, remove when label = null\n",
    "\n",
    "test_data = test_data[test_data[\"label\"].isna()==False]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7358157",
   "metadata": {},
   "source": [
    "#### Add purchase_freq feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05820de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"purchase_freq\"] = (train_data[\"days_since_first_purchase\"\n",
    "                                         ]-train_data[\"days_since_last_purchase\"\n",
    "                                                     ])/train_data['num_of_transactions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c594f92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[\"purchase_freq\"] = (test_data[\"days_since_first_purchase\"\n",
    "                                         ]-test_data[\"days_since_last_purchase\"\n",
    "                                                     ])/test_data['num_of_transactions']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3b6f56",
   "metadata": {},
   "source": [
    "## EXPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71679805",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df = train_data.reset_index()\n",
    "import pickle \n",
    "f = open(\"OM_D1_train_data\", 'wb') \n",
    "pickle.dump(train_data_df, f)\n",
    "f.close()\n",
    "\n",
    "test_data_df = test_data.reset_index()\n",
    "import pickle \n",
    "f = open(\"OM_D1_test_data\", 'wb') \n",
    "pickle.dump(test_data_df, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c13d359",
   "metadata": {},
   "source": [
    "# Dataset export for networkx graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50df82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_feature_period_end_date-train_data_feature_period_start_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72412627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the transaction data according to dates\n",
    "\n",
    "train_df_nx = data_df[(data_df[\"TransactionDate\"]> train_data_feature_period_start_date) &\n",
    "                    (data_df[\"TransactionDate\"]< train_data_feature_period_end_date)].reset_index(drop=True) #sadece featurelar\n",
    "\n",
    "\n",
    "# --------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "test_df_nx = data_df[(data_df[\"TransactionDate\"]> test_data_feature_period_start_date) &\n",
    "                    (data_df[\"TransactionDate\"]< test_data_feature_period_end_date)].reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad29fbe9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df_nx.groupby(\"CustomerID\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a9068c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_df_nx[[\"Ean\",\"ProductCode\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea21bee",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Take ean and customerID columns to build a network\n",
    "\n",
    "data_nx_NF = train_df_nx.copy()\n",
    "data_nx_NF = data_nx_NF[[\"CustomerID\", \"Ean\"]]\n",
    "\n",
    "# Drop null values\n",
    "\n",
    "data_nx_NF.dropna(inplace=True)\n",
    "data_nx_NF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e008a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert EAN values to appropriate dtype\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def convert_to_int(x):\n",
    "    try:\n",
    "        return int(x)\n",
    "    except (ValueError, TypeError):\n",
    "        return x\n",
    "\n",
    "data_nx_NF[\"Ean\"] = data_nx_NF[\"Ean\"].apply(convert_to_int)\n",
    "data_nx_NF[\"Ean\"] = data_nx_NF[\"Ean\"].transform(str)\n",
    "data_nx_NF[\"Ean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272cf396",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nx_NF.groupby(\"CustomerID\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fc34b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nx_NF.reset_index(inplace=True, drop=True)\n",
    "import pickle \n",
    "fl = open(\"OM_D3_dataset-network\", 'wb') \n",
    "pickle.dump(data_nx_NF, fl)\n",
    "fl.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
