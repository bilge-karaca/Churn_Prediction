{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1825cced",
   "metadata": {},
   "source": [
    "# Feature Engineering: Time-based-features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b9b629",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle \n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9f92d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data\n",
    "data_df = pd.read_csv(\"private_dataset.txt\", encoding='unicode_escape', sep=\"\\t\")\n",
    "\n",
    "f = open('OM_D1_train_data', 'rb')\n",
    "dfx = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f805f453",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df[\"TransactionDate\"] = pd.to_datetime(data_df[\"TransactionDate\"], format=\"%Y-%m-%d %H:%M:%S\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19d2094",
   "metadata": {},
   "source": [
    "### Filter the dataset to avoid data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee78903e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train data FEATURE period\n",
    "\n",
    "train_data_feature_period_start_date = pd.to_datetime(\"2021-03-01 00:00:00\")\n",
    "train_data_feature_period_end_date = pd.to_datetime(\"2022-06-01 00:00:00\") \n",
    "\n",
    "# --------------------------------------------------------\n",
    "\n",
    "# TEST data FEATURE period\n",
    "\n",
    "test_data_feature_period_start_date = pd.to_datetime(\"2021-09-01 00:00:00\") \n",
    "test_data_feature_period_end_date = pd.to_datetime(\"2022-12-01 00:00:00\")\n",
    "\n",
    "# --------------------------------------------------------\n",
    "\n",
    "data_df = data_df[data_df.TransactionDate < train_data_feature_period_end_date]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2c9adf",
   "metadata": {},
   "source": [
    "interpurchase time. x day between 1st and 2nd purchase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdaacc7",
   "metadata": {},
   "source": [
    "### Execute all the changes that we've made on the omnichannel dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f5da05",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cols = [\"Insert\", \"43\", \"different\", \"column\", \"names\", \"here\"]\n",
    "\n",
    "data_df.columns = new_cols\n",
    "data_df.columns \n",
    "data_df.drop(['ProductID_2','Phone', 'Email', 'Email2', 'Gender', 'Payment_Info','tbr'],\n",
    "             axis=1,inplace=True) #ProductID kolonu ile aynı seyleri iceriyor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de0500c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bask_data_omni = data_df[['CustomerID', 'TransactionDate', 'Price',\n",
    "                          'Discount_Amount', 'Net_Amount', 'Quantity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d29b686",
   "metadata": {},
   "outputs": [],
   "source": [
    "bask_data_omni = bask_data_omni.groupby(['CustomerID', 'TransactionDate']).sum()\n",
    "bask_data_omni = bask_data_omni.reset_index()\n",
    "sorted_df_omni = bask_data_omni.sort_values(by=['CustomerID', 'TransactionDate'],ascending=False).dropna()\n",
    "sorted_df_omni.reset_index(inplace=True, drop=True)\n",
    "sorted_df_omni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f09de3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df_omni=sorted_df_omni[sorted_df_omni.Price>0]\n",
    "sorted_df_omni"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb417e0",
   "metadata": {},
   "source": [
    "### Create new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa525a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df_omni[\"discount_perc\"]= (sorted_df_omni[\"DiscountAmount\"]/sorted_df_omni[\"Price\"])\n",
    "sorted_df_omni.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6db4db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df_omni[\"days_diff_omni\"] = np.nan\n",
    "sorted_df_omni[\"is_1st_purchase_omni\"] = np.nan\n",
    "sorted_df_omni[\"delta_basket_val\"] = np.nan\n",
    "sorted_df_omni[\"delta_basket_unit\"] = np.nan\n",
    "sorted_df_omni[\"delta_discount_perc\"] = np.nan\n",
    "sorted_df_omni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a89c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate time between consecutive purchases\n",
    "\n",
    "for i in range(len(sorted_df_omni.CustomerID)):\n",
    "    if i != len(sorted_df_omni.CustomerID)-1:\n",
    "        if sorted_df_omni.CustomerID[i] == sorted_df_omni.CustomerID[i+1]:\n",
    "            sorted_df_omni[\"days_diff_omni\"][i] = sorted_df_omni.TransactionDate[i]-sorted_df_omni.TransactionDate[i+1]\n",
    "            sorted_df_omni[\"is_1st_purchase_omni\"][i] = 0 \n",
    "\n",
    "\n",
    "        else:\n",
    "            sorted_df_omni[\"days_diff_omni\"][i] = np.nan \n",
    "            sorted_df_omni[\"is_1st_purchase_omni\"][i] = 1\n",
    "\n",
    "\n",
    "    else:\n",
    "        sorted_df_omni[\"days_diff_omni\"][i]=np.nan\n",
    "        sorted_df_omni[\"is_1st_purchase_omni\"][i] = np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a89d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate time between consecutive purchases\n",
    "\n",
    "for i in range(len(sorted_df_omni.CustomerID)):\n",
    "    if i != len(sorted_df_omni.CustomerID)-1:\n",
    "        if sorted_df_omni.CustomerID[i] == sorted_df_omni.CustomerID[i+1]:\n",
    "            \n",
    "            sorted_df_omni[\"delta_basket_val\"][i] = sorted_df_omni.Net_Amount[i]-sorted_df_omni.Net_Amount[i+1]\n",
    "            sorted_df_omni[\"delta_basket_unit\"][i] = sorted_df_omni.quantity[i]-sorted_df_omni.Quantity[i+1]\n",
    "            sorted_df_omni[\"delta_discount_perc\"][i] = sorted_df_omni.discount_perc[i]-sorted_df_omni.discount_perc[i+1]\n",
    "\n",
    "        else:\n",
    "            sorted_df_omni[\"delta_basket_val\"][i] = 0\n",
    "            sorted_df_omni[\"delta_basket_unit\"][i] = 0\n",
    "            sorted_df_omni[\"delta_discount_perc\"][i] = 0\n",
    "\n",
    "    else:\n",
    "        sorted_df_omni[\"delta_basket_val\"][i] = 0\n",
    "        sorted_df_omni[\"delta_basket_unit\"][i] = 0\n",
    "        sorted_df_omni[\"delta_discount_perc\"][i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6a825b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert timedelta to days_diff\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "sorted_df_omni.days_diff_omni = sorted_df_omni.days_diff_omni.apply(lambda x: x.days if isinstance(x, timedelta) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520921dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_df_omni = sorted_df_omni[sorted_df_omni[\"is_1st_purchase_omni\"]==0].groupby(\"CustomerID\").agg(['mean',\n",
    "                                                                                                    'min',\n",
    "                                                                                                    'max'])\n",
    "\n",
    "cust_df_omni.drop([\"TransactionDate\", \"is_1st_purchase_omni\"],axis=1, inplace=True)\n",
    "\n",
    "D8_final_df = cust_df_omni.merge(dfx, how=\"right\", on=\"CustomerID\")\n",
    "\n",
    "D8_final_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35060176",
   "metadata": {},
   "outputs": [],
   "source": [
    "D8_final_df.drop(['index', 'Category3', 'isContactable', 'City','last_coupon_type_used',\n",
    "                         'isShippedToBilled_sum', 'Device', 'num_of_purchases_w_discount'],\n",
    "                 axis=1,\n",
    "                 inplace=True)\n",
    "\n",
    "df = D8_final_df.copy()\n",
    "\n",
    "\n",
    "df[\"Age\"] = df.loc[:,\"Age\"].apply(lambda x: np.where(x<18,np.nan,x))\n",
    "df[\"Age\"] = df.loc[:,\"Age\"].apply(lambda x: np.where(x>100,np.nan,x))\n",
    "\n",
    "\n",
    "df[\"purchase_freq\"].replace(0,np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f8137a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier Removal\n",
    "\n",
    "colns = [\"avg_order_value\",'Category2', 'Category4', \"num_returns\", \"offline\",\n",
    "         'Category1', 'Axe1','Axe2', 'Axe3', 'Axe4',\"avg_discount_amount\",\"avg_basket_size\"]\n",
    "\n",
    "colns = colns + list(df.columns[1:28])\n",
    "\n",
    "for col in colns:\n",
    "    print(df[col].apply(lambda x: x > df[col].mean() + 6*df[col].std()).sum(),\"outliers, 6 std far from the mean\")\n",
    "    print(df[col].apply(lambda x: x > df[col].mean() + 6*df[col].std()).sum() /df[col].count(),\"of the column\")\n",
    "    print(\"Dropping...\")\n",
    "    df.drop(df[df[col].apply(lambda x: x > df[col].mean() + 6*df[col].std())==True].index, inplace=True)\n",
    "    print(\"Dropped.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216c62c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"purchase_period\"] = df[\"purchase_freq\"]\n",
    "df[\"purchase_freq\"] = 1/df[\"purchase_freq\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994f7442",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "colns = [\"purchase_freq\", \"purchase_period\"]\n",
    "\n",
    "for col in colns:\n",
    "    print(df[col].apply(lambda x: x > df[col].mean() + 6*df[col].std()).sum(),\"outliers, 6 std far from the mean\")\n",
    "    print(df[col].apply(lambda x: x > df[col].mean() + 6*df[col].std()).sum() /df[col].count(),\"of the column\")\n",
    "    print(\"Dropping...\")\n",
    "    df.drop(df[df[col].apply(lambda x: x > df[col].mean() + 6*df[col].std())==True].index, inplace=True)\n",
    "    print(\"Dropped.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d57feaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb5ebcf",
   "metadata": {},
   "source": [
    "# Draw histograms with all the 27 new features created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f804d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc('xtick', labelsize=7) \n",
    "plt.rc('ytick', labelsize=7)\n",
    "df.hist(figsize = [30,30], bins=30);\n",
    "# bu datada customer'a ait ilk alışverişler yok, yeni eklenen feature'larda."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d1fc21",
   "metadata": {},
   "source": [
    "### Check correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b293473",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set()\n",
    "plt.figure(figsize=(25,25))\n",
    "plt.rc('xtick', labelsize=20) \n",
    "plt.rc('ytick', labelsize=20)\n",
    "plt.rcParams.update({'font.size': 10})\n",
    "sns.heatmap(round(df.corr(),2), annot=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a13adbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop([('Price', 'mean'), ('Price', 'min'),('Price', 'max'),'ZPL1'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887dafa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a183b3c5",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47afd9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab06759",
   "metadata": {},
   "outputs": [],
   "source": [
    "X= df.drop([\"CustomerID\",\"label\"],axis=1)\n",
    "y= df[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfba113",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731fc0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18433813",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6da814",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns\n",
    "\n",
    "new_cols = [    \"('DiscountAmount', 'mean')\",       \"('DiscountAmount', 'min')\",\n",
    "            \"('DiscountAmount', 'max')\",           \"('NetAmount', 'mean')\",\n",
    "                 \"('NetAmount', 'min')\",            \"('NetAmount', 'max')\",\n",
    "                \"('quantity', 'mean')\",           \" ('quantity', 'min')\",\n",
    "                  \"('quantity', 'max')\",       \"('discount_perc', 'mean')\",\n",
    "              \"('discount_perc', 'min')\",        \"('discount_perc', 'max')\",\n",
    "            \"('days_diff_omni', 'mean')\",      \" ('days_diff_omni', 'min')\",\n",
    "            \"('days_diff_omni', 'max')\",   \"('delta_basket_val', 'mean')\",\n",
    "           \"('delta_basket_val', 'min')\",     \"('delta_basket_val', 'max')\",\n",
    "         \"('delta_basket_unit', 'mean')\",   \" ('delta_basket_unit', 'min')\",\n",
    "          \"('delta_basket_unit', 'max')\", \"('delta_discount_perc', 'mean')\",\n",
    "        \"('delta_discount_perc', 'min')\",  \"('delta_discount_perc', 'max')\",\n",
    "                 'num_of_transactions',               'avg_basket_size',\n",
    "                  'num_of_total_items',               'min_basket_size',\n",
    "                     'max_basket_size',           'avg_discount_amount',\n",
    "            'lifetime_discount_amount',               'avg_order_value',\n",
    "                    'total_net_amount',      'days_since_last_purchase',\n",
    "           'days_since_first_purchase',                   'num_returns',\n",
    "                           'Category1',                         'Category2',\n",
    "                           'Category4',                           'Axe1',\n",
    "                                'Axe2',                          'Axe3',\n",
    "                                 'Age',                       'offline',\n",
    "                       'purchase_freq',               'purchase_period']\n",
    "\n",
    "X.columns=new_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26f2e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "num_vars = list(X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2f7ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"pipe_cat = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"one_hot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\"\"\"\n",
    "pipe_num = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "ct = ColumnTransformer([\n",
    "    #(\"categorical_vars\", pipe_cat, cat_vars),\n",
    "    (\"numeric_vars\", pipe_num, num_vars),\n",
    "    \n",
    "], remainder=\"passthrough\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a0f03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d807bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = df.copy()\n",
    "import pickle \n",
    "fx = open(\"OM_D8_Train_Data_Clean\", 'wb') \n",
    "pickle.dump(clean_data, fx)\n",
    "fx.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6d2fba",
   "metadata": {},
   "source": [
    "# Results Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc37cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, precision_recall_curve\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, roc_auc_score, f1_score\n",
    "from sklearn.metrics import accuracy_score, average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d9c4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data frame to store the results\n",
    "def print_results(headline, true_value, pred, probs):\n",
    "    scores=[]\n",
    "    CM = confusion_matrix(true_value, pred)\n",
    "    scores.append(headline)\n",
    "    scores.append(accuracy_score(true_value, pred))      #accuracy\n",
    "    scores.append(int(CM[1,1]))                          #TP\n",
    "    scores.append(int(CM[0,1]))                          #FP\n",
    "    scores.append(int(CM[0][0]))                         #TN\n",
    "    scores.append(int(CM[1][0]))                         #FN\n",
    "    scores.append(precision_score(true_value, pred))     #precision\n",
    "    scores.append(recall_score(true_value, pred))        #recall\n",
    "    scores.append(roc_auc_score(true_value, probs))      #roc_auc\n",
    "    p, r, _ = precision_recall_curve(true_value, probs) \n",
    "    scores.append(auc(r,p))                              #pr_auc\n",
    "    scores.append(f1_score(true_value, pred, average=\"macro\"))            #f1-score\n",
    "    return scores\n",
    "\n",
    "score_names = ['method','accuracy','TP','FP','TN','FN','precision','recall','roc_auc','pr_auc','f1']\n",
    "dfAcc = pd.DataFrame(data=np.zeros(shape=(0,11)), columns = score_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e3ef40",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90aa1148",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491cf0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE, SMOTENC, RandomOverSampler\n",
    "from imblearn.pipeline import Pipeline as pipe_imb\n",
    "from imblearn.over_sampling import SMOTE, SMOTENC, RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "from imblearn.pipeline import make_pipeline as make_pipeline_imb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200cc677",
   "metadata": {},
   "source": [
    "# LoR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4aca73",
   "metadata": {},
   "outputs": [],
   "source": [
    "ovsmp_pipe = pipe_imb([('ct'        , ct),\n",
    "                       ('sampler'   , RandomOverSampler(random_state=42,\n",
    "                                                       sampling_strategy = 0.6)),\n",
    "                       ('classifier', LogisticRegression(random_state=42, max_iter=100000))\n",
    "                          ]\n",
    "                   )\n",
    "\n",
    "params = [\n",
    "\n",
    "\n",
    "            {'classifier__C'      :[0.05,0.1,0.2], \n",
    "           'classifier__penalty': ['l1'],\n",
    "           'classifier__solver' : ['liblinear', 'saga'],\n",
    "            'sampler__sampling_strategy': [0.3,0.4,0.5], },\n",
    "\n",
    "\n",
    "          {'classifier__C'      : [0.05,0.1,0.2], \n",
    "           'classifier__penalty': ['l2'], \n",
    "           'classifier__solver' : ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga'],\n",
    "            'sampler__sampling_strategy': [0.3,0.4,0.5],\n",
    "          }]\n",
    "\n",
    "         \n",
    "\n",
    "# train/validation with the same ratio of classes\n",
    "kfold = StratifiedKFold(n_splits = 4, random_state = 42, shuffle=True) \n",
    "\n",
    "grid = GridSearchCV(ovsmp_pipe, \n",
    "                    param_grid = params, \n",
    "                    cv = kfold, \n",
    "                    scoring = 'f1_macro', \n",
    "                    verbose = 1,\n",
    "                    n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edb87c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Run the grid search \n",
    "grid.fit(X_train, y_train)\n",
    "print(grid.best_estimator_,'\\n')\n",
    "print('Best parameters  :', grid.best_params_)\n",
    "print('\\nTraining F1_macro:', grid.score(X_train, y_train))\n",
    "print('Test F1_macro :', grid.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd6032b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probs = grid.predict_proba(X_test)[:,1]\n",
    "scores = print_results('D3_LoR_new-feats', y_test, grid.predict(X_test), pred_probs)\n",
    "dftmp = pd.DataFrame([scores], columns=score_names)\n",
    "dfAcc = pd.concat([dfAcc, dftmp], ignore_index=True)\n",
    "dfAcc[['TP','FP', 'TN', 'FN']] = dfAcc[['TP','FP', 'TN', 'FN']].astype(int)\n",
    "dfAcc.style.hide_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9893c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_macro(y_true, y_pred):\n",
    "    return f1_score(y_true, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03db47ff",
   "metadata": {},
   "source": [
    "# XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1816e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGB\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "ovsmp_pipe = pipe_imb([('ct'        , ct),\n",
    "                       ('sampler'   , RandomOverSampler(random_state=42,\n",
    "                                                       sampling_strategy=0.6)),\n",
    "                       ('classifier', XGBClassifier(objective='binary:logistic', \n",
    "                                                    eval_metric = f1_macro,\n",
    "                                                    n_estimators=1000, \n",
    "                                                    eta=0.01, # default 0.3\n",
    "                                                    max_depth=3, \n",
    "                                                    subsample=0.9, \n",
    "                                                    min_child_weight=25, \n",
    "                                                    gamma=5, \n",
    "                                                    reg_lambda=1, \n",
    "                                                    alpha=3, \n",
    "                                                    colsample_bytree=0.5, \n",
    "                                                    #colsample_bylevel=0.5,\n",
    "                                                   # scale_pos_weight = 0.35\n",
    "                                                   )\n",
    "                                                   )\n",
    "                            ]\n",
    "                   )\n",
    "\n",
    "\n",
    "\n",
    "params = [{\n",
    "          #  'sampler__sampling_strategy': [0.3,0.5,0.6,0.8],\n",
    "          #  'classifier__n_estimators':[750,1000,1500],\n",
    "          # 'classifier__eta': [0.01,0.001,0.0001]\n",
    "            'classifier__max_depth':[2],\n",
    "           'classifier__min_child_weight': [100],\n",
    "         #  'classifier__colsample_bytree':[0.5],\n",
    "          # 'classifier__subsample' : [0.5,0.7,0.9],\n",
    "          # 'classifier__alpha':[1,3,5], #  defult 0. Increasing this value will make model more conservative.\n",
    "          #  'classifier__gamma':[1,3,5], #  defult 0. Increasing this value will make model more conservative.\n",
    "           # 'classifier__reg_lambda':[5], #  def=1 .Increasing this value will make model more conservative.\n",
    "         #   'classifier__scale_pos_weight' : [1]\n",
    "}\n",
    "         ]\n",
    "\n",
    "\n",
    "\n",
    "# train/validation with the same ratio of classes\n",
    "kfold = StratifiedKFold(n_splits = 4, random_state = 42, shuffle=True) \n",
    "\n",
    "grid = GridSearchCV(ovsmp_pipe, \n",
    "                    param_grid = params, \n",
    "                    cv = kfold, \n",
    "                    scoring = 'f1_macro', \n",
    "                    verbose = 1,\n",
    "                    n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1198d2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Run the grid search \n",
    "grid.fit(X_train, y_train)\n",
    "print(grid.best_estimator_,'\\n')\n",
    "print('Best parameters  :', grid.best_params_)\n",
    "print('\\nTraining F1 score:', grid.score(X_train, y_train))\n",
    "print(\"Test F1 score:\", grid.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7061a597",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probs = grid.predict_proba(X_test)[:,1]\n",
    "scores = print_results('D3_XGB_new-feats', y_test, grid.predict(X_test), pred_probs)\n",
    "dftmp = pd.DataFrame([scores], columns=score_names)\n",
    "dfAcc = pd.concat([dfAcc, dftmp], ignore_index=True)\n",
    "dfAcc[['TP','FP', 'TN', 'FN']] = dfAcc[['TP','FP', 'TN', 'FN']].astype(int)\n",
    "dfAcc.style.hide_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d50358",
   "metadata": {},
   "source": [
    "# LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e06c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "# LIGHTGBM\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "ovsmp_pipe = pipe_imb([('ct'        , ct),\n",
    "                       ('sampler'   , RandomOverSampler(random_state=42,\n",
    "                                                       sampling_strategy=0.5)),\n",
    "                       ('LGBM',   lgb.LGBMClassifier(objective=\"binary\",\n",
    "                                                   #  class_weight=\"balanced\",                                                   \n",
    "                                                     n_estimators=1000, \n",
    "                                                     learning_rate=0.01,\n",
    "                                                     min_child_weight=25, \n",
    "                                                      max_depth=6, \n",
    "                                                     num_leaves=25, \n",
    "                                                     min_child_samples=25,                                                      \n",
    "                                                     reg_alpha=1, \n",
    "                                                     reg_lambda=1,\n",
    "                                                 #    subsample_freq=0, \n",
    "                                                     colsample_bytree=0.8, \n",
    "                                                     subsample=0.7,\n",
    "                                                     min_split_gain = 25,\n",
    "                                                      min_data_in_leaf = 25,\n",
    "                                                     random_state=42))\n",
    "                          ]\n",
    "                   )\n",
    "\n",
    "\n",
    "param_grid = [{\n",
    "                #  'sampler__sampling_strategy': [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1],\n",
    "             #        'LGBM__n_estimators': [750,1000,1300,1500], \n",
    "              #   'LGBM__learning_rate' : [0.0001,0.001,0.01],\n",
    "              #    'LGBM__min_child_weight': [60], #\n",
    "                  'LGBM__max_depth' : [2,3,4],          #\n",
    "         #        'LGBM__num_leaves': [500], \n",
    "                 'LGBM__min_child_samples': [25,50,75],                                                      \n",
    "                 'LGBM__reg_alpha' : [1,3], # default 0 \n",
    "              #   'LGBM__min_data_in_leaf' : [50],\n",
    "                 'LGBM__reg_lambda' : [1,3], # default 0 \n",
    "              #   'LGBM__subsample_freq' : [1,5,10,100,500], \n",
    "              # 'LGBM__subsample' : [0.5,0.6,0.7,0.8,0.9], \n",
    "              # 'LGBM__colsample_bytree' : [0.1], \n",
    "            #    'LGBM__min_split_gain' : [50], #\n",
    "}\n",
    "         ]\n",
    "\n",
    "\n",
    "# train/validation with the same ratio of classes\n",
    "kfolds = StratifiedKFold(n_splits=4, shuffle=True, random_state=42)\n",
    "\n",
    "grid = GridSearchCV(ovsmp_pipe, \n",
    "                    param_grid = param_grid, \n",
    "                    cv = kfolds, \n",
    "                    scoring = 'f1_macro', \n",
    "                    verbose = 2,\n",
    "                    n_jobs = -1)\n",
    "# default lgbm = 0.57"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3f3914",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Run the grid search \n",
    "grid.fit(X_train, y_train)\n",
    "print(grid.best_estimator_,'\\n')\n",
    "print('Best parameters  :', grid.best_params_)\n",
    "print('\\nTraining F1 score:', grid.score(X_train, y_train))\n",
    "print(\"Test F1 score:\", grid.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fb84d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7d03fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probs = grid.predict_proba(X_test)[:,1]\n",
    "scores = print_results('D3_LGBM_new-feats', y_test, grid.predict(X_test), pred_probs)\n",
    "dftmp = pd.DataFrame([scores], columns=score_names)\n",
    "dfAcc = pd.concat([dfAcc, dftmp], ignore_index=True)\n",
    "dfAcc[['TP','FP', 'TN', 'FN']] = dfAcc[['TP','FP', 'TN', 'FN']].astype(int)\n",
    "dfAcc.style.hide_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88795358",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAcc.drop(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8090b137",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a5b20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "    \n",
    "ovsmp_pipe = pipe_imb([('ct'        , ct),\n",
    "                       ('sampler'   , RandomOverSampler(random_state=42,\n",
    "                                                       sampling_strategy = 0.6)),\n",
    "                       ('classifier', SVC(random_state=42,\n",
    "                                          C=0.05, #default 1\n",
    "                                          kernel='rbf',\n",
    "                                          gamma='scale',\n",
    "                                          probability=True))\n",
    "                          ]\n",
    "                   )\n",
    "\n",
    "params = [\n",
    "    {         \n",
    "           # 'sampler__sampling_strategy': [0.5,0.6,0.7,0.8],\n",
    "          #  'classifier__C': [0.05,0.1,0.2],\n",
    "           #  'classifier__kernel': ['rbf'],\n",
    "           # 'classifier__gamma' : ['scale']\n",
    "      #    },\n",
    "         #   {\n",
    "        #    'sampler__sampling_strategy': [0.5,0.6,0.7],\n",
    "         #   'classifier__C': [0.1,0.3,0.5],\n",
    "        #     'classifier__kernel': ['poly'],\n",
    "        #    'classifier__degree': [2,3,4],\n",
    "        #    'classifier__gamma' : ['scale', 'auto']\n",
    "          }\n",
    "    ]\n",
    "\n",
    "# train/validation with the same ratio of classes\n",
    "kfold = StratifiedKFold(n_splits = 4, random_state = 42, shuffle=True) \n",
    "\n",
    "grid = GridSearchCV(ovsmp_pipe, \n",
    "                    param_grid = params, \n",
    "                    cv = kfold, \n",
    "                    scoring = 'f1_macro', \n",
    "                    verbose = 1,\n",
    "                    n_jobs = -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b161b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Run the grid search \n",
    "grid.fit(X_train, y_train)\n",
    "print(grid.best_estimator_,'\\n')\n",
    "print('Best parameters  :', grid.best_params_)\n",
    "print('\\nTraining F1 score:', grid.score(X_train, y_train))\n",
    "print(\"Test F1 score:\", grid.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574bfac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probs = grid.predict_proba(X_test)[:,1]\n",
    "scores = print_results('D3_SVM_new-feats', y_test, grid.predict(X_test), pred_probs)\n",
    "dftmp = pd.DataFrame([scores], columns=score_names)\n",
    "dfAcc = pd.concat([dfAcc, dftmp], ignore_index=True)\n",
    "dfAcc[['TP','FP', 'TN', 'FN']] = dfAcc[['TP','FP', 'TN', 'FN']].astype(int)\n",
    "dfAcc.style.hide_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f66036c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAcc_pck = dfAcc.copy()\n",
    "import pickle \n",
    "fd = open(\"OM_D3_results_table-new_feats\", 'wb') \n",
    "pickle.dump(dfAcc_pck, fd)\n",
    "fd.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
